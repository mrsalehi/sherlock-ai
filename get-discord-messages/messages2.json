[{"id":"1117187070532792414","type":0,"content":"<@1072591948499664996>  Help me with code to fetch docs using mmr using FAISS vectorstore and load_qa_chain","channel_id":"1072944049788555314","author":{"id":"780667203967254529","username":"btisback","global_name":null,"avatar":"c1140d00a052cf98d2ccd22cdc5ad6bf","discriminator":"5898","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T20:22:33.947000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117187070532792414","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Help me with code to fetch docs using mmr using FAISS vectorstore and load_qa_chain","last_message_id":"1117187285704769599","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T20:22:34.277000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T20:22:34.277000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","780667203967254529"]}},{"id":"1117184291114340412","type":0,"content":"<@1072591948499664996> I have this code:\n```\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)```\n\nAnd I get this:\n\n```ValidationError: 1 validation error for LLMChain\nprompt\n  value is not a valid dict (type=type_error.dict)\n```","channel_id":"1072944049788555314","author":{"id":"845460624712007691","username":"fernandomaytorena","global_name":null,"avatar":"80071cf0e62641e8e07591c1878de98a","discriminator":"6629","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T20:11:31.282000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117184291114340412","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have this code:```from langchain.chat_models import ChatOpenAIllm = ChatOpenAI(model_","last_message_id":"1117184422945501336","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T20:11:31.446000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T20:11:31.446000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","845460624712007691","1072591948499664996"]}},{"id":"1117184120674594847","type":0,"content":"<@1072591948499664996>","channel_id":"1072944049788555314","author":{"id":"845460624712007691","username":"fernandomaytorena","global_name":null,"avatar":"80071cf0e62641e8e07591c1878de98a","discriminator":"6629","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T20:10:50.646000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1117169270980354198","type":0,"content":"<@1072591948499664996> I am using MultiPromptChain to determine how to best respond to a response. How should I handle the scenario where no response is necessary? For example when the response indicates that the conversation is over?","channel_id":"1072944049788555314","author":{"id":"1058241473813938208","username":"Fima","global_name":null,"avatar":null,"discriminator":"1892","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T19:11:50.203000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117169270980354198","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am using MultiPromptChain to determine how to best respond to a response. How should I h","last_message_id":"1117169344653307904","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T19:11:50.415000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T19:11:50.415000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1058241473813938208","1072591948499664996"]}},{"id":"1117160011718737921","type":0,"content":"<@1072591948499664996> is it possible to view the final formatted prompt including context variables before it gets sent to the LLM?","channel_id":"1072944049788555314","author":{"id":"955842062639583313","username":"Rusty Shackleford","global_name":null,"avatar":"db2851fe179715b2bfd6ab045dd6ec48","discriminator":"4883","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T18:35:02.623000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117160011718737921","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"is it possible to view the final formatted prompt including context variables before it ge","last_message_id":"1117160123513712740","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T18:35:02.944000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T18:35:02.944000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","955842062639583313"]}},{"id":"1117152869490098177","type":0,"content":"<@1072591948499664996> Update this code so that it uses the ChatOpenAI LLM instead of the OpenAI LLM.\n```\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List\nfrom rich import print, print_json, inspect, pretty\n\npretty.install()\n\n\nmodel_name = 'text-davinci-003'\ntemperature = 0.0\nmodel = OpenAI(model_name=model_name, temperature=temperature)\n\n# Define your desired data structure.\n\n\nclass Joke(BaseModel):\n    setup: str = Field(description=\"question to set up a joke\")\n    punchline: str = Field(description=\"answer to resolve the joke\")\n\n    # You can add custom validation logic easily with Pydantic.\n    @validator('setup')\n    def question_ends_with_question_mark(cls, field):\n        if field[-1] != '?':\n            raise ValueError(\"Badly formed question!\")\n        return field\n\n\n# And a query intented to prompt a language model to populate the data structure.\njoke_query = \"Tell me a joke.\"\n\n# Set up a parser + inject instructions into the prompt template.\nparser = PydanticOutputParser(pydantic_object=Joke)\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\n_input = prompt.format_prompt(query=joke_query)\n\noutput = model(_input.to_string())\n\nparser.parse(output)\n```","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T18:06:39.783000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117152869490098177","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Update this code so that it uses the ChatOpenAI LLM instead of the OpenAI LLM.```from la","last_message_id":"1117162007683158126","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T18:06:39.956000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T18:06:39.956000+00:00"},"message_count":23,"member_count":4,"rate_limit_per_user":0,"flags":0,"total_message_sent":23,"member_ids_preview":["1072591948499664996","437808476106784770","454324403161923604","861013720170758154"]}},{"id":"1117150296473358437","type":0,"content":"<@1072591948499664996> how can i get the embedding of an entry in a collection?","channel_id":"1072944049788555314","author":{"id":"776898092125257809","username":"tom_lrd","global_name":null,"avatar":"b12873b9ff32189142f2bd7b4bbe8e70","discriminator":"7035","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:56:26.328000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117150296473358437","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i get the embedding of an entry in a collection?","last_message_id":"1117165725132529744","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T17:56:26.468000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T17:56:26.468000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","776898092125257809","437808476106784770"]}},{"id":"1117148901636591656","type":0,"content":"<@1072591948499664996> I'm using RetrievalQA, but my results are often cut off, do I need to set the max token or something else ?","channel_id":"1072944049788555314","author":{"id":"524207009122222080","username":"kiritoku_","global_name":null,"avatar":null,"discriminator":"2045","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:50:53.773000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117148901636591656","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I'm using RetrievalQA, but my results are often cut off, do I need to set the max token or","last_message_id":"1117151310215659583","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T17:50:53.943000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T17:50:53.943000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["524207009122222080","437808476106784770","1072591948499664996"]}},{"id":"1117148323015565383","type":0,"content":"<@&1072943855747481672> convert the following Python into Javascript: ```qa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=index.vectorstore.as_retriever(), \n    verbose=True,\n    chain_type_kwargs = {\n        \"document_separator\": \"<<<<>>>>>\"\n    }\n)```","channel_id":"1072944049788555314","author":{"id":"629819041996865536","username":"JSDevGuy","global_name":null,"avatar":null,"discriminator":"6518","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:48:35.819000+00:00","edited_timestamp":"2023-06-10T17:52:36.183000+00:00","flags":0,"components":[]},{"id":"1117138405663457290","type":0,"content":"<@1072591948499664996> how do I ingest a pdf into a vector database but not by page but by chapter and sub chapter?","channel_id":"1072944049788555314","author":{"id":"104329886482575360","username":"darkbelg","global_name":"Darkbelg the 500th","avatar":"cbd0e91cba0203c2f274b885122b3ef6","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:09:11.338000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117138405663457290","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I ingest a pdf into a vector database but not by page but by chapter and sub chapte","last_message_id":"1117148638993461329","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T17:09:11.539000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T17:09:11.539000+00:00"},"message_count":21,"member_count":4,"rate_limit_per_user":0,"flags":0,"total_message_sent":21,"member_ids_preview":["1072591948499664996","104329886482575360","437808476106784770","697612541123362826"]}},{"id":"1117137671928676463","type":0,"content":"<@1072591948499664996> How can I query a pinecone vectorstore based on metadata","channel_id":"1072944049788555314","author":{"id":"738523681071956060","username":"agentricity","global_name":null,"avatar":"b2722e0e38b0082f717395ba6b64d8c5","discriminator":"5860","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:06:16.402000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117137671928676463","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I query a pinecone vectorstore based on metadata","last_message_id":"1117137833543602246","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T17:06:16.626000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T17:06:16.626000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","738523681071956060","1072591948499664996"]}},{"id":"1117137475614277742","type":0,"content":"<@1072591948499664996> What is the advantage of using LLMChain instead of simply calling the model","channel_id":"1072944049788555314","author":{"id":"697612541123362826","username":"KaiTakami","global_name":null,"avatar":"c795b2536c4c3e6b4b9354955acadcfd","discriminator":"9891","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T17:05:29.597000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117137475614277742","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What is the advantage of using LLMChain instead of simply calling the model","last_message_id":"1117139362413559952","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T17:05:29.752000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T17:05:29.752000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["437808476106784770","697612541123362826","1072591948499664996"]}},{"id":"1117131391654690836","type":0,"content":"<@1072591948499664996> What is the difference between OpenAIChat and ChatOpenAI? (in the JS library)","channel_id":"1072944049788555314","author":{"id":"1037348918486372443","username":"ItayElgazar","global_name":null,"avatar":"8fea09a4b37dca1813b174355e6e9c51","discriminator":"3359","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T16:41:19.068000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117131391654690836","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What is the difference between OpenAIChat and ChatOpenAI? (in the JS library)","last_message_id":"1117131978941141113","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T16:41:19.234000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T16:41:19.234000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","1037348918486372443","437808476106784770"]}},{"id":"1117121589482356849","type":0,"content":"<@1072591948499664996> How to search for several related results for a specific document and then incorporate them into a prompt to organize the response?","channel_id":"1072944049788555314","author":{"id":"524207009122222080","username":"kiritoku_","global_name":null,"avatar":null,"discriminator":"2045","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T16:02:22.048000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117121589482356849","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to search for several related results for a specific document and then incorporate the","last_message_id":"1117121783582175252","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T16:02:22.808000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T16:02:22.808000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["524207009122222080","437808476106784770","1072591948499664996"]}},{"id":"1117120439571980398","type":0,"content":"<@&1072943855747481672> how can I instantiate a RefineDocumentsChain without load_qa_from_sources_chain in python?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T15:57:47.888000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1117089596409385001","type":0,"content":"<@1072591948499664996> how can i track token usage in nodejs","channel_id":"1072944049788555314","author":{"id":"393477907709689862","username":"Tobi","global_name":null,"avatar":"94f9492f1d2fc64df8e1922475facc1b","discriminator":"1449","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:55:14.305000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117089596409385001","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i track token usage in nodejs","last_message_id":"1117089645931528212","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:55:14.579000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:55:14.579000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","393477907709689862","1072591948499664996"]}},{"id":"1117084709172813914","type":0,"content":"<@1072591948499664996> load_prompt is always giving this error while loading a .yaml file\n\"[Errno 2] No such file or directory: 'data/prompts/discuss/thought_examples.yaml'\"","channel_id":"1072944049788555314","author":{"id":"1101776408507863150","username":"innotone","global_name":null,"avatar":null,"discriminator":"8935","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:35:49.097000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117084709172813914","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"load_prompt is always giving this error while loading a .yaml file\"[Errno 2] No such file","last_message_id":"1117085499677483118","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:35:49.276000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:35:49.276000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","1101776408507863150","437808476106784770"]}},{"id":"1117080906180603974","type":0,"content":"<@1072591948499664996> what is the difference between chains and chain types? how do chains use chain types? are there prompts declared inside chain types?","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:20:42.393000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117080906180603974","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between chains and chain types? how do chains use chain types? are","last_message_id":"1117081962700619869","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:20:42.877000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:20:42.877000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","437808476106784770","839215717693259816"]}},{"id":"1117080801952153660","type":0,"content":"<@1072591948499664996> how do i use load prompt from file in google colab","channel_id":"1072944049788555314","author":{"id":"1101776408507863150","username":"innotone","global_name":null,"avatar":null,"discriminator":"8935","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:20:17.543000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117080801952153660","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i use load prompt from file in google colab","last_message_id":"1117081295122616381","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:20:17.834000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:20:17.834000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","1101776408507863150"]}},{"id":"1117079692000903199","type":0,"content":"<@1072591948499664996> what's the difference between import { OpenAI } from 'langchain/llms/openai' and import { ChatOpenAI } from 'langchain/chat_models/openai'","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:15:52.910000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117079692000903199","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what's the difference between import { OpenAI } from 'langchain/llms/openai' and import {","last_message_id":"1117102585824952401","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:15:53.051000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:15:53.051000+00:00"},"message_count":15,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":15,"member_ids_preview":["437808476106784770","1072591948499664996","706926250723115019"]}},{"id":"1117075745643188304","type":0,"content":"<@1072591948499664996> What are pros and cons of FAISS and Chromadb vectorstores in Python?","channel_id":"1072944049788555314","author":{"id":"1043191328734974032","username":"Illia Herasymenko","global_name":null,"avatar":"b8e40df311dc27b2fb12e4dafd27dc36","discriminator":"8452","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T13:00:12.025000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117075745643188304","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What are pros and cons of FAISS and Chromadb vectorstores in Python?","last_message_id":"1117075850496593950","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T13:00:12.194000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T13:00:12.194000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1043191328734974032","1072591948499664996","437808476106784770"]}},{"id":"1117073021501444176","type":0,"content":"<@1072591948499664996> i have written a promt in .yaml file in my computer. I am trying to execute a llm chain in google colab.  I have uploaded the file in colab environment. How do i load it to use in the prompt.","channel_id":"1072944049788555314","author":{"id":"1101776408507863150","username":"innotone","global_name":null,"avatar":null,"discriminator":"8935","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:49:22.539000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117073021501444176","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i have written a promt in .yaml file in my computer. I am trying to execute a llm chain in","last_message_id":"1117076001739001867","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:49:22.906000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:49:22.906000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","1101776408507863150","437808476106784770"]}},{"id":"1117072590972911626","type":0,"content":"<@1072591948499664996> What is the most lightweight vectorstore that I can use locally?","channel_id":"1072944049788555314","author":{"id":"1043191328734974032","username":"Illia Herasymenko","global_name":null,"avatar":"b8e40df311dc27b2fb12e4dafd27dc36","discriminator":"8452","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:47:39.893000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117072590972911626","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What is the most lightweight vectorstore that I can use locally?","last_message_id":"1117072737865826344","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:47:40.013000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:47:40.013000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1043191328734974032","1072591948499664996","437808476106784770"]}},{"id":"1117070697647321158","type":0,"content":"<@1072591948499664996> when i try to run npm install langchain i get # npm resolution error report\n\nWhile resolving: koretex-chatbot-ui@0.1.0\nFound: react@18.2.0\nnode_modules/react\n  react@\"^18.2.0\" from the root project\n\nCould not resolve dependency:\npeer react@\"^16.13.1 || ^17.0.1\" from @supabase/ui@0.36.5\nnode_modules/@supabase/ui\n  @supabase/ui@\"^0.36.5\" from the root project\n\nFix the upstream dependency conflict, or retry\nthis command with --force or --legacy-peer-deps\nto accept an incorrect (and potentially broken) dependency resolution. What should i do","channel_id":"1072944049788555314","author":{"id":"938959997604335676","username":"alpha","global_name":null,"avatar":"510f96d4778214c24bbd4e2c52637c2c","discriminator":"4769","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:40:08.489000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117070697647321158","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when i try to run npm install langchain i get # npm resolution error reportWhile resolvi","last_message_id":"1117070762851962880","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:40:08.835000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:40:08.835000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","938959997604335676","437808476106784770"]}},{"id":"1117067456343703572","type":0,"content":"<@1072591948499664996> i am getting Cannot find module 'langchain/llms/openai' or its corresponding type declarations.","channel_id":"1072944049788555314","author":{"id":"938959997604335676","username":"alpha","global_name":null,"avatar":"510f96d4778214c24bbd4e2c52637c2c","discriminator":"4769","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:27:15.702000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117067456343703572","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i am getting Cannot find module 'langchain/llms/openai' or its corresponding type declarat","last_message_id":"1117070633730326579","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:27:15.849000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:27:15.849000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["938959997604335676","437808476106784770","1072591948499664996"]}},{"id":"1117062180878168125","type":0,"content":"hi <@1072591948499664996>  im wondering which vector store to choose? Pinecone, Chroma and Postgres pgvector seem to be the dominant choices. Im inclined to go with PGvector cause I know postgres and i can use a solid DBAAS like supabase - does anyone have any comments on this?","channel_id":"1072944049788555314","author":{"id":"168844554286399488","username":"fotoflo","global_name":"fotoflo","avatar":"17ac4dd69d915d4c19ce3e1787b971d8","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:06:17.933000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117062180878168125","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"hi   im wondering which vector store to choose? Pinecone, Chroma and Postgres pgvector see","last_message_id":"1117062997932785705","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:06:18.066000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:06:18.066000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","168844554286399488","1072591948499664996"]}},{"id":"1117061627695595631","type":0,"content":"<@1072591948499664996> i don't get why for example the ConversationChain has a prompt parameter that i can modify but why the ConversationalRetrievalChain doesn't","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T12:04:06.044000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117061627695595631","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i don't get why for example the ConversationChain has a prompt parameter that i can modify","last_message_id":"1117062797960957982","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T12:04:06.251000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T12:04:06.251000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["839215717693259816","1072591948499664996","437808476106784770"]}},{"id":"1117057522201796649","type":0,"content":"<@1072591948499664996> in python langchain, whats the difference between chain_type map_reduce and other chain_types?","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T11:47:47.218000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117057522201796649","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"in python langchain, whats the difference between chain_type map_reduce and other chain_ty","last_message_id":"1117062066876993647","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T11:47:47.436000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T11:47:47.436000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","719286426063339571"]}},{"id":"1117042771702849546","type":0,"content":"<@1072591948499664996>  \nhttps://github.com/hwchase17/langchainjs\nHas anyone installed jest on top of this?\nMy application actually runs but i keep getting errors with jest \n```\n FAIL  src/__test__/index.test.ts\n  ‚óè Test suite failed to run\n\n    Cannot find module 'langchain' from 'src/index.ts'\n```","channel_id":"1072944049788555314","author":{"id":"168844554286399488","username":"fotoflo","global_name":"fotoflo","avatar":"17ac4dd69d915d4c19ce3e1787b971d8","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[{"type":"article","url":"https://github.com/hwchase17/langchainjs","title":"GitHub - hwchase17/langchainjs","description":"Contribute to hwchase17/langchainjs development by creating an account on GitHub.","color":1975079,"provider":{"name":"GitHub"},"thumbnail":{"url":"https://opengraph.githubassets.com/1751801721c7f69fc766fde20e0ab236f000b94eb2e40ef62a30c8efad03049b/hwchase17/langchainjs","proxy_url":"https://images-ext-1.discordapp.net/external/rtMAoPWQCoqDYmfI-nDfVGF-TrpXW8DI7DLhktaXljo/https/opengraph.githubassets.com/1751801721c7f69fc766fde20e0ab236f000b94eb2e40ef62a30c8efad03049b/hwchase17/langchainjs","width":1200,"height":600}}],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T10:49:10.425000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117042771702849546","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"https://github.com/hwchase17/langchainjsHas anyone installed jest on top of this?My appl","last_message_id":"1117042848483778630","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T10:49:10.878000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T10:49:10.878000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","168844554286399488"]}},{"id":"1117020240220520499","type":0,"content":"<@1072591948499664996> When using the `CHAT_CONVERSATIONAL_REACT_DESCRIPTION` agent with tools and history, somehow in the prompt templates history ends up the string `Human: TOOLS`.\nHeres the prompt:\n```\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n    \nHuman: Hi\nAI: Hello, how can I help you?\nHuman: What is the current supply of DFI?\nAI: The current total supply \nof DFI is 546,205,671 DFI.\nHuman: Thank you!\nAI: You are welcome. If there is anything else I can help you with, feel free to ask.\nHuman: What was my first message to you?\nAI: Hi\nHuman: And what was your response to that?\nAI: The circulating supply of DFI can change frequently, so it's best to check a reliable cryptocurrency market data website for the most up-to-date information.\nHuman: Hi\nAI: Hello! How can I assist you today?\nHuman: TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n```\n\nWhy is `Human: TOOLS` in there? I don't want it.","channel_id":"1072944049788555314","author":{"id":"248893598370758666","username":"0ptim","global_name":"0ptim","avatar":"e0917f199b994f01e8aa91baef12ff4f","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T09:19:38.501000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117020240220520499","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"When using the `CHAT_CONVERSATIONAL_REACT_DESCRIPTION` agent with tools and history, someh","last_message_id":"1117020707361140800","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T09:19:38.626000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T09:19:38.626000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","248893598370758666","1072591948499664996"]}},{"id":"1117013344742813798","type":0,"content":"<@1072591948499664996> Is it somehow possible to adjust the model settings (like changing the temp) without instantiating a new instance of OpenAI?","channel_id":"1072944049788555314","author":{"id":"1037348918486372443","username":"ItayElgazar","global_name":null,"avatar":"8fea09a4b37dca1813b174355e6e9c51","discriminator":"3359","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T08:52:14.491000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1117013344742813798","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is it somehow possible to adjust the model settings (like changing the temp) without insta","last_message_id":"1117131103971573871","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T08:52:14.669000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T08:52:14.669000+00:00"},"message_count":22,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":24,"member_ids_preview":["1037348918486372443","1072591948499664996","437808476106784770"]}},{"id":"1116996136780189726","type":0,"content":"<@1072591948499664996> how to use RecursiveCharacterTextSplitter","channel_id":"1072944049788555314","author":{"id":"903953122643300412","username":"trungnx05","global_name":"typera","avatar":null,"discriminator":"8069","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T07:43:51.793000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116996136780189726","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to use RecursiveCharacterTextSplitter","last_message_id":"1116999593721344121","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T07:43:51.922000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T07:43:51.922000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","437808476106784770","903953122643300412"]}},{"id":"1116996068828270642","type":0,"content":"<@1072591948499664996> <@1072591948499664996> I want you to tell me the difference, in langchain, between a llm, a chain, a chat model and an agent. I want you to describe it to me with pseudo code as well for each term, what is going on behind the hoods and how do those things differ from each other?","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T07:43:35.592000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116996068828270642","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I want you to tell me the difference, in langchain, between a llm, a chain, a chat model a","last_message_id":"1117061471453593660","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T07:43:35.783000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T07:43:35.783000+00:00"},"message_count":25,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":25,"member_ids_preview":["1072591948499664996","437808476106784770","839215717693259816"]}},{"id":"1116989141549334619","type":0,"content":"<@1072591948499664996> is it possible to have 2 sets of embeddings","channel_id":"1072944049788555314","author":{"id":"379318534070861834","username":"boredchilli","global_name":null,"avatar":"a716c7bb5109a3949ecebec081715ce3","discriminator":"9051","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T07:16:04+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116989141549334619","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"is it possible to have 2 sets of embeddings","last_message_id":"1116990191199076433","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T07:16:04.159000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T07:16:04.159000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["379318534070861834","1072591948499664996","437808476106784770"]}},{"id":"1116985133338214512","type":0,"content":"<@1072591948499664996> How to persist ConversationEntityMemory","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T07:00:08.368000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116985133338214512","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to persist ConversationEntityMemory","last_message_id":"1116994093034242109","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T07:00:08.546000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T07:00:08.546000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["719286426063339571","437808476106784770","1072591948499664996"]}},{"id":"1116984333337636914","type":0,"content":"<@1072591948499664996> what is chunking?","channel_id":"1072944049788555314","author":{"id":"810778533654954005","username":"Eden M","global_name":null,"avatar":"66cacc0db8860c9b4ba54212326562f6","discriminator":"2475","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:56:57.633000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116984333337636914","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is chunking?","last_message_id":"1116984426388275200","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:56:57.816000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:56:57.816000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","810778533654954005","437808476106784770"]}},{"id":"1116983937273704499","type":0,"content":"<@1072591948499664996> how do I integrate pinecone into my python app?","channel_id":"1072944049788555314","author":{"id":"810778533654954005","username":"Eden M","global_name":null,"avatar":"66cacc0db8860c9b4ba54212326562f6","discriminator":"2475","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:55:23.204000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116983937273704499","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I integrate pinecone into my python app?","last_message_id":"1116984182095224842","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:55:23.500000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:55:23.500000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","810778533654954005"]}},{"id":"1116983459840270426","type":0,"content":"<@1072591948499664996> how do I integrate pinecone into my app?","channel_id":"1072944049788555314","author":{"id":"810778533654954005","username":"Eden M","global_name":null,"avatar":"66cacc0db8860c9b4ba54212326562f6","discriminator":"2475","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:53:29.375000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116983459840270426","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I integrate pinecone into my app?","last_message_id":"1116983745602392104","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:53:29.491000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:53:29.491000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","810778533654954005"]}},{"id":"1116982706576838748","type":0,"content":"<@1072591948499664996> which vectorstore should be used","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:50:29.783000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116982706576838748","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"which vectorstore should be used","last_message_id":"1116984087173939260","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:50:29.963000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:50:29.963000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["719286426063339571","437808476106784770","1072591948499664996"]}},{"id":"1116978144298147840","type":0,"content":"<@1072591948499664996> what is the session id in Postgres chat history","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:32:22.051000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116978144298147840","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the session id in Postgres chat history","last_message_id":"1116983740745400413","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:32:22.206000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:32:22.206000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","437808476106784770","719286426063339571"]}},{"id":"1116977339079852044","type":0,"content":"<@1072591948499664996> Can VectorStoreRetrieverMemory also write chat history?","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:29:10.072000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116977339079852044","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Can VectorStoreRetrieverMemory also write chat history?","last_message_id":"1116977976643440650","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T06:29:10.341000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T06:29:10.341000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["437808476106784770","719286426063339571","1072591948499664996"]}},{"id":"1116971346593857586","type":0,"content":"<@&1072943855747481672> Can VectorStoreRetrieverMemory also write chat history?","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:05:21.352000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116970418641850389","type":0,"content":"hm","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:01:40.111000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116970399427743789","type":0,"content":"<@&1072943855747481672>  <@&1072943855747481672> Can VectorStoreRetrieverMemory also write chat history? What is the best Vectorstore? Can I use DeepLake?","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:01:35.530000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116970290212253776","type":0,"content":"<@&1072943855747481672> Can VectorStoreRetrieverMemory also write chat history? What is the best Vectorstore? Can I use DeepLake?","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T06:01:09.491000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116965990102736967","type":0,"content":"<@1072591948499664996> which document loaders can parse scanned pdfs?","channel_id":"1072944049788555314","author":{"id":"838372646294782002","username":"gopher","global_name":null,"avatar":"65c143e1c7a9bdff7ddbee0c265ba4ad","discriminator":"4086","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T05:44:04.265000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116965990102736967","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"which document loaders can parse scanned pdfs?","last_message_id":"1116997655680274453","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T05:44:04.388000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T05:44:04.388000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["838372646294782002","1072591948499664996","437808476106784770"]}},{"id":"1116940869900193812","type":0,"content":"<@1072591948499664996> How can I change the parameters of an LLM after it's been loaded?","channel_id":"1072944049788555314","author":{"id":"229576111212855296","username":"coffeevampir3","global_name":"Coffee Vampire","avatar":"303d86ba731d14668c4b46b6dae38ef9","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T04:04:15.142000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116940869900193812","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I change the parameters of an LLM after it's been loaded?","last_message_id":"1116940959998017577","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T04:04:15.401000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T04:04:15.401000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","229576111212855296"]}},{"id":"1116923297062465599","type":0,"content":"<@1072591948499664996> translate the langchain tool written in python below to JavaScript\n\n```python\nfrom langchain.tools import BaseTool\nfrom math import pi\nfrom typing import Union\n  \n\nclass CircumferenceTool(BaseTool):\n      name = \"Circumference calculator\"\n      description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n\n    def _run(self, radius: Union[int, float]):\n        return float(radius)*2.0*pi\n\n    def _arun(self, radius: int):\n        raise NotImplementedError(\"This tool does not support async\")\n```","channel_id":"1072944049788555314","author":{"id":"231174216693710848","username":"thedeadmanwalking","global_name":"Deadman","avatar":"c0cf4d4779b64c0621542a8b89e9f579","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T02:54:25.451000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116923297062465599","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"translate the langchain tool written in python below to JavaScript```pythonfrom langcha","last_message_id":"1116925976056692857","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T02:54:25.655000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T02:54:25.655000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["231174216693710848","1072591948499664996","437808476106784770"]}},{"id":"1116908327339831377","type":0,"content":"<@1072591948499664996>  - how to pass user query into load_summarize_chain?","channel_id":"1072944049788555314","author":{"id":"839889127616938064","username":"robyn","global_name":null,"avatar":"cb81fd6d4d543ba4295397d51ac1048f","discriminator":"2087","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T01:54:56.391000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116908327339831377","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"- how to pass user query into load_summarize_chain?","last_message_id":"1116909187344105502","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T01:54:56.664000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T01:54:56.664000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","839889127616938064","1072591948499664996"]}},{"id":"1116902347013099571","type":0,"content":"<@1072591948499664996> \"TypeError: langchain.chains.base.Chain.run() got multiple values for keyword argument 'input_documents'\" for code \"docs = docsearch.similarity_search(question)\nresult = chain({\"input_documents\": docs[0], \"question\": question}, return_only_outputs=True)\"","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T01:31:10.570000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116902347013099571","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"\"TypeError: langchain.chains.base.Chain.run() got multiple values for keyword argument 'in","last_message_id":"1116970468809900072","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T01:31:10.748000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T01:31:10.748000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["719286426063339571","1072591948499664996","437808476106784770"]}},{"id":"1116899054169899050","type":0,"content":"<@&1072943855747481672> Do I need to create a custom wrapper to work with openAI gpt4? Is there a reason to create one?","channel_id":"1072944049788555314","author":{"id":"1116878156272500767","username":"Rosey Posey","global_name":null,"avatar":null,"discriminator":"0182","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T01:18:05.495000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116898834673586288","type":0,"content":"<@&1072943855747481672> Do I need to create a custom wrapper to work with openAI gpt4?e","channel_id":"1072944049788555314","author":{"id":"1116878156272500767","username":"Rosey Posey","global_name":null,"avatar":null,"discriminator":"0182","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T01:17:13.163000+00:00","edited_timestamp":"2023-06-10T01:17:22.076000+00:00","flags":0,"components":[]},{"id":"1116893268337250334","type":0,"content":"<@1072591948499664996> How do I use gpt-3.5-turbo as a llm to create a sql agent","channel_id":"1072944049788555314","author":{"id":"244181430085746688","username":"tdw","global_name":"tony","avatar":"e71969006b2f81ff9d210fc1f2dbed16","discriminator":"0","public_flags":4194432,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-10T00:55:06.045000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116893268337250334","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I use gpt-3.5-turbo as a llm to create a sql agent","last_message_id":"1116894633235070976","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-10T00:55:06.252000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-10T00:55:06.252000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","244181430085746688"]}},{"id":"1116864118108979230","type":0,"content":"<@1072591948499664996>  VectorstoreIndexCreator is there a Javascript equivalent of this? If so where can i find it?","channel_id":"1072944049788555314","author":{"id":"629819041996865536","username":"JSDevGuy","global_name":null,"avatar":null,"discriminator":"6518","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T22:59:16.089000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116864118108979230","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"VectorstoreIndexCreator is there a Javascript equivalent of this? If so where can i find i","last_message_id":"1116864173054373899","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T22:59:16.228000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T22:59:16.228000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","629819041996865536","1072591948499664996"]}},{"id":"1116862277954588682","type":0,"content":"<@1072591948499664996> what is the difference between RetrievalQAWithSourcesChain and load_qa_with_source_chains","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T22:51:57.362000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116862277954588682","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between RetrievalQAWithSourcesChain and load_qa_with_source_chains","last_message_id":"1116863017922076682","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T22:51:57.493000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T22:51:57.493000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["719286426063339571","1072591948499664996","437808476106784770"]}},{"id":"1116854529800020019","type":0,"content":"<@1072591948499664996> I have the following line of code:\n\n```\nchain = RetrievalQAWithSourcesChain.from_chain_type(ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", request_timeout=120), chain_type=\"map_reduce\", retriever=compression_retriever)\n```\n\nI'm getting this Attribute Error:\n\n```AttributeError: 'function' object has no attribute 'from_chain_type'```","channel_id":"1072944049788555314","author":{"id":"845460624712007691","username":"fernandomaytorena","global_name":null,"avatar":"80071cf0e62641e8e07591c1878de98a","discriminator":"6629","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T22:21:10.058000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116854529800020019","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have the following line of code:```chain = RetrievalQAWithSourcesChain.from_chain_typ","last_message_id":"1116855017434001479","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T22:21:10.203000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T22:21:10.203000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["845460624712007691","1072591948499664996","437808476106784770"]}},{"id":"1116846842584502422","type":0,"content":"<@1072591948499664996> Can langchain work with starcoder models>?","channel_id":"1072944049788555314","author":{"id":"738523681071956060","username":"agentricity","global_name":null,"avatar":"b2722e0e38b0082f717395ba6b64d8c5","discriminator":"5860","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T21:50:37.283000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116846842584502422","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Can langchain work with starcoder models>?","last_message_id":"1116847006376267897","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T21:50:37.527000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T21:50:37.527000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","738523681071956060"]}},{"id":"1116839872418824213","type":0,"content":"<@1072591948499664996> \nI am importing documents into a Chroma vectorstore from a list but after the first 5 documents, I get the error listed below. How do I fix this error?\n```python\nembeddings = HuggingFaceEmbeddings()\nvectordb = Chroma(collection_name=\"docs\", persist_directory=\"DynamicLangchainDB\", embedding_function=embeddings)\nvectordb.persist()\n\nfor doc in os.listdir(pdfs):\n    print(\"Loading \" + doc + \"...\", end=\"\")\n    try:\n        loader = PyPDFLoader(f\"{pdfs}/{doc}\")\n        pages = loader.load_and_split()\n        vectordb.add_documents(pages)\n        print(\"Done!\")\n    except:\n        print(\"Failed!\")\n```\n```\nTraceback (most recent call last):\n  File \"C:/Users/Mike/Desktop/DynamicDocLangchain.py\", line 26, in <module>\n    vectordb.add_documents(pages)\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\vectorstores\\base.py\", line 62, in add_documents\n    return self.add_texts(texts, metadatas, **kwargs)\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\vectorstores\\chroma.py\", line 160, in add_texts\n    self._collection.add(\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 82, in add\n    ids = validate_ids(maybe_cast_one_to_many(ids))\n  File \"C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\chromadb\\api\\types.py\", line 71, in maybe_cast_one_to_many\n    if isinstance(target[0], (int, float)):\nIndexError: list index out of range\n```","channel_id":"1072944049788555314","author":{"id":"231174216693710848","username":"thedeadmanwalking","global_name":"Deadman","avatar":"c0cf4d4779b64c0621542a8b89e9f579","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T21:22:55.466000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116839872418824213","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am importing documents into a Chroma vectorstore from a list but after the first 5 docum","last_message_id":"1116840064316620931","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T21:22:55.708000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T21:22:55.708000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","231174216693710848"]}},{"id":"1116838479876325440","type":0,"content":"<@1072591948499664996> How can I run a langchain agent executor?","channel_id":"1072944049788555314","author":{"id":"789533134911045632","username":"Cartier","global_name":null,"avatar":"2a7ee6f45ab69950f0bd97f7a2728c81","discriminator":"5356","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T21:17:23.458000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116838479876325440","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I run a langchain agent executor?","last_message_id":"1116838767324569650","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T21:17:23.700000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T21:17:23.700000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","1072591948499664996","789533134911045632"]}},{"id":"1116836277019484171","type":0,"content":"<@&1072943855747481672> How do you instantiate a custom STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent? I want to change its prompt","channel_id":"1072944049788555314","author":{"id":"359842104798871563","username":"yungseneca","global_name":null,"avatar":"3d049fc03743b85f529dfe952acde880","discriminator":"8481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T21:08:38.256000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116833397839499274","type":0,"content":"<@1072591948499664996> is there a way to ask back user thing required ti run a query while running instructions given in prompt?","channel_id":"1072944049788555314","author":{"id":"1063461577463963759","username":"Kiran","global_name":null,"avatar":null,"discriminator":"9558","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:57:11.806000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116833397839499274","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"is there a way to ask back user thing required ti run a query while running instructions g","last_message_id":"1116841490820382750","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:57:11.921000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:57:11.921000+00:00"},"message_count":6,"member_count":4,"rate_limit_per_user":0,"flags":0,"total_message_sent":6,"member_ids_preview":["437808476106784770","1072591948499664996","1063461577463963759","359842104798871563"]}},{"id":"1116829744533618799","type":0,"content":"<@1072591948499664996> come faccio a limitare la lunghezza delle risponste in una conversational chain?","channel_id":"1072944049788555314","author":{"id":"236536688082419713","username":"Nikazzio","global_name":"Nikazzio","avatar":"7ca3fa8c668c42b01016ba5eb2d0b0de","discriminator":"0584","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:42:40.790000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116829744533618799","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"come faccio a limitare la lunghezza delle risponste in una conversational chain?","last_message_id":"1116830306075430962","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:42:40.910000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:42:40.910000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["236536688082419713","437808476106784770","1072591948499664996"]}},{"id":"1116826330470809610","type":0,"content":"<@1072591948499664996> I have this custom tool: \n```location_tool = Tool(name = \"Location Finder\", description=\"Useful for when you need to answer questions about products, services or stores. Input should be the customer's language and the chat history.\", func=location.run, return_direct=True)````\n\nBut the chain I am using expects 2 inputs. This it my chain:\n```location_template = \"\"\"Your goal is to ask the customer's location! Don't forget to be kind!\nYour question should be in {language}.\n\nCustomer conversation:\n{chat_history}\n\"\"\"\nlocation = LLMChain(llm=llm, prompt=PromptTemplate.from_template(location_template))```\nHow can I pass multiple inputs in this chain? Could you please fix my code?","channel_id":"1072944049788555314","author":{"id":"620349234633375797","username":"pcoronado","global_name":null,"avatar":"85d9a016581cbff6f9ee8f15930f1539","discriminator":"4338","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:29:06.814000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116826330470809610","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have this custom tool: ```location_tool = Tool(name = \"Location Finder\", description=\"U","last_message_id":"1116826476625535046","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:29:07.013000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:29:07.013000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","620349234633375797"]}},{"id":"1116826324087083088","type":0,"content":"<@1072591948499664996> hoe do i update a faiss index with new content?","channel_id":"1072944049788555314","author":{"id":"863755342408318976","username":"Uke__Uke","global_name":null,"avatar":"b5999d838d28a5140e6bfde91eca70a2","discriminator":"1472","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:29:05.292000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116826324087083088","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"hoe do i update a faiss index with new content?","last_message_id":"1116826737137959053","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:29:05.481000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:29:05.481000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","863755342408318976","437808476106784770"]}},{"id":"1116825796770803772","type":0,"content":"<@1072591948499664996> what's the max tokens i can set with gpt-4","channel_id":"1072944049788555314","author":{"id":"782729216461897798","username":"vintro","global_name":null,"avatar":"df80ad1878ebab3dcd6ca590a8064884","discriminator":"1873","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:26:59.570000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116825796770803772","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what's the max tokens i can set with gpt-4","last_message_id":"1116825911094935623","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:26:59.794000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:26:59.794000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["782729216461897798","1072591948499664996","437808476106784770"]}},{"id":"1116825545670397995","type":0,"content":"<@1072591948499664996> I have this custom tool: \n```\nlocation_tool = Tool(name = \"Location Finder\", description=\"Useful for when you need to answer questions about products, services or stores. Input should be the customer's language and the chat history.\", func=location.run, return_direct=True)\n```\nBut the chain I am using expects 2 inputs. This it the chain:\n```\nlocation_template = \"\"\"Your goal is to ask the customer's location! Don't forget to be kind!\nYour question should be in {language}.\n\nCustomer conversation:\n{chat_history}\n\"\"\"\nlocation = LLMChain(llm=llm, prompt=PromptTemplate.from_template(location_template))\n```\nHow can I pass multiple tools in this chain? Could you please fix my code?","channel_id":"1072944049788555314","author":{"id":"620349234633375797","username":"pcoronado","global_name":null,"avatar":"85d9a016581cbff6f9ee8f15930f1539","discriminator":"4338","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T20:25:59.703000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116825545670397995","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have this custom tool: ```location_tool = Tool(name = \"Location Finder\", description=\"","last_message_id":"1116825761563807894","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T20:25:59.956000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T20:25:59.956000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","620349234633375797"]}},{"id":"1116816292939317288","type":0,"content":"<@1072591948499664996> how can I make it so my chatbot gives suggestion query options in javascript","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T19:49:13.680000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116816292939317288","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I make it so my chatbot gives suggestion query options in javascript","last_message_id":"1116816369816719390","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T19:49:13.835000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T19:49:13.835000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1107656108199395328","1072591948499664996","437808476106784770"]}},{"id":"1116815406850650213","type":0,"content":"<@1072591948499664996> how do I get a ConversationChain using ConversationBufferMemory to work when my chat ui on one server, makes a request to my app server first, and then my app server makes a request to OpenAI? It appears that I only have 1 Human/AI message in memory because the memory is cleared during every request/response cycle","channel_id":"1072944049788555314","author":{"id":"903818640510562354","username":"DeFiDivvy","global_name":null,"avatar":"6dcb81614e960f8e1b14f33f00caf559","discriminator":"6958","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T19:45:42.420000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116815406850650213","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I get a ConversationChain using ConversationBufferMemory to work when my chat ui on","last_message_id":"1116877160813166653","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T19:45:42.940000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T19:45:42.940000+00:00"},"message_count":19,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":19,"member_ids_preview":["903818640510562354","1072591948499664996","437808476106784770"]}},{"id":"1116810648198852719","type":0,"content":"<@1072591948499664996> when using a RetrievalQAWithSourcesChain as a Tool for an Agent, how can I pass the sources back to the Agent?","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T19:26:47.869000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116810648198852719","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when using a RetrievalQAWithSourcesChain as a Tool for an Agent, how can I pass the source","last_message_id":"1116812156890005554","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T19:26:48.037000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T19:26:48.037000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","437808476106784770","251035647689621504"]}},{"id":"1116810552950403213","type":0,"content":"<@1072591948499664996>","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T19:26:25.160000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116810362562547802","type":0,"content":"<@1072591948499664996> when using a RetrialQAWithSourcesChain as a Tool for an Agent, how can I return the sources?","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T19:25:39.768000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116810362562547802","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when using a RetrialQAWithSourcesChain as a Tool for an Agent, how can I return the source","last_message_id":"1116810545123836024","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T19:25:39.928000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T19:25:39.928000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["251035647689621504","1072591948499664996","437808476106784770"]}},{"id":"1116799629992939610","type":0,"content":"<@1072591948499664996> How to connect to a hugging face inference endpoint and use it a llm?","channel_id":"1072944049788555314","author":{"id":"1057969139932729344","username":"Fred B","global_name":null,"avatar":null,"discriminator":"1660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T18:43:00.924000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116799629992939610","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to connect to a hugging face inference endpoint and use it a llm?","last_message_id":"1116800402042667070","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T18:43:01.092000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T18:43:01.092000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","1057969139932729344","1072591948499664996"]}},{"id":"1116794285522686042","type":0,"content":"<@1072591948499664996> how do I create a chat model with memory on a document?","channel_id":"1072944049788555314","author":{"id":"1015774039156985896","username":"satvikp","global_name":null,"avatar":null,"discriminator":"3132","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T18:21:46.703000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116794285522686042","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I create a chat model with memory on a document?","last_message_id":"1116794731914068119","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T18:21:46.926000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T18:21:46.926000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1015774039156985896","1072591948499664996","437808476106784770"]}},{"id":"1116790919115587638","type":0,"content":"<@1072591948499664996> Using RecursiveCharacterTextSplitter with chunksize=155, overlapping=100 and following seperators [\"\\n\\n\", \".\", \",\", \" \", \"\"] I geht many short chunks of size 50 to 100. Is there a way to specify a minimum chunk size?","channel_id":"1072944049788555314","author":{"id":"705361636084547584","username":"Alioio","global_name":null,"avatar":null,"discriminator":"5869","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T18:08:24.089000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116790919115587638","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Using RecursiveCharacterTextSplitter with chunksize=155, overlapping=100 and following sep","last_message_id":"1116791023264354334","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T18:08:24.293000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T18:08:24.293000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","705361636084547584","1072591948499664996"]}},{"id":"1116781744310464522","type":0,"content":"<@1072591948499664996> Best way to use an agent optimized for conversation that can also access documents in chroma db?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T17:31:56.645000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116781744310464522","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Best way to use an agent optimized for conversation that can also access documents in chro","last_message_id":"1116781971687886859","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T17:31:56.786000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T17:31:56.786000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["161206345620652032","1072591948499664996","437808476106784770"]}},{"id":"1116777079179059241","type":0,"content":"<@1072591948499664996> I know that the max token limit for gpt 3.5 is 4096, but I want to use a free model, in particular falcon-7b-instrubt, but I can't seem to find the max token limit for the model. do you know where I might be able to find it","channel_id":"1072944049788555314","author":{"id":"140594162050400256","username":"quickandsmart","global_name":"QuickAndSmart","avatar":"5a4933b2a4b09df6893ddbac79ba3e9c","discriminator":"0","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T17:13:24.391000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116777079179059241","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I know that the max token limit for gpt 3.5 is 4096, but I want to use a free model, in pa","last_message_id":"1117099667973484605","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T17:13:24.643000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T17:13:24.643000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","437808476106784770","140594162050400256"]}},{"id":"1116772939459735622","type":0,"content":"<@1072591948499664996> if I want to view the {agent_scratchpad} variable, how can I do this in python?","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T16:56:57.405000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116772939459735622","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"if I want to view the {agent_scratchpad} variable, how can I do this in python?","last_message_id":"1116773965730426923","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T16:56:57.712000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T16:56:57.712000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["1072591948499664996","251035647689621504","437808476106784770"]}},{"id":"1116770093720940544","type":0,"content":"<@1072591948499664996> I am subclassing a `StructuredTool` in order to add my own custom logic to a custom tool. However, I am getting an error that the `func` field is required, even though I thought using a StructuredTool would avoid this issue. I am using Python. What can I do to fix this?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T16:45:38.928000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116770093720940544","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am subclassing a `StructuredTool` in order to add my own custom logic to a custom tool.","last_message_id":"1117120129893937324","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T16:45:39.234000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T16:45:39.234000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["359130672872423464","437808476106784770","1072591948499664996"]}},{"id":"1116764049695326363","type":0,"content":"<@1072591948499664996>  So... I have an index in pinecone and I inserted my texts there so they are there as my knowledge base (or at least that was my intention), in the codes available in the docs I didn't see about just \"getting\" data from pinecone and ask questions on top of that, just saw this ```from langchain.vectorstores import Pinecone\n\ntext_field = \"text\"\n\n# switch back to normal index for langchain\nindex = pinecone.Index(index_name)\n\nvectorstore = Pinecone(\n    index, embed.embed_query, text_field\n)```, but I don't want to send any more text to my index, I just want to query it.","channel_id":"1072944049788555314","author":{"id":"991112954567213197","username":"Yasmim Rosa","global_name":null,"avatar":"65e3b922f5680aac52531fe3df4f1c0a","discriminator":"0785","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T16:21:37.920000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116764049695326363","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"So... I have an index in pinecone and I inserted my texts there so they are there as my kn","last_message_id":"1116764726920237137","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T16:21:38.076000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T16:21:38.076000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["991112954567213197","1072591948499664996","437808476106784770"]}},{"id":"1116762568292315206","type":0,"content":"<@1072591948499664996>  write code to parse an online github repo and add as the file ulrs as \"source\" metadata","channel_id":"1072944049788555314","author":{"id":"863755342408318976","username":"Uke__Uke","global_name":null,"avatar":"b5999d838d28a5140e6bfde91eca70a2","discriminator":"1472","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T16:15:44.726000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116762568292315206","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"write code to parse an online github repo and add as the file ulrs as \"source\" metadata","last_message_id":"1117051085463506984","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T16:15:44.904000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T16:15:44.904000+00:00"},"message_count":19,"member_count":4,"rate_limit_per_user":0,"flags":0,"total_message_sent":22,"member_ids_preview":["863755342408318976","218035266970058752","1072591948499664996","437808476106784770"]}},{"id":"1116755053575938148","type":0,"content":"<@1072591948499664996> how can I pass the `ignore_agent` flag to a callback_handler?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:45:53.078000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116755053575938148","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I pass the `ignore_agent` flag to a callback_handler?","last_message_id":"1117120736390295634","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:45:53.209000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:45:53.209000+00:00"},"message_count":11,"member_count":4,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["1072591948499664996","1063461577463963759","359130672872423464","437808476106784770"]}},{"id":"1116753387879403520","type":0,"content":"<@1072591948499664996> How can I deploy a langchain app using unstructured file loader in Javascript?","channel_id":"1072944049788555314","author":{"id":"697612541123362826","username":"KaiTakami","global_name":null,"avatar":"c795b2536c4c3e6b4b9354955acadcfd","discriminator":"9891","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:39:15.945000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116753387879403520","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I deploy a langchain app using unstructured file loader in Javascript?","last_message_id":"1116756354057646221","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:39:16.146000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:39:16.146000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","697612541123362826","437808476106784770"]}},{"id":"1116752564130697266","type":0,"content":"<@1072591948499664996> db = SQLDatabase.from_uri(db_uri) is giving me the error \"ValidationError                           Traceback (most recent call last)\nCell In[60], line 1\n----> 1 toolkit = SQLDatabaseToolkit(db=db)\n\nFile ~/.pyenv/versions/3.11.3/envs/venv_bearweb/lib/python3.11/site-packages/pydantic/main.py:342, in pydantic.main.BaseModel.__init__()\n\nValidationError: 1 validation error for SQLDatabaseToolkit\nllm\n  field required (type=value_error.missing)\"","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:35:59.548000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116752564130697266","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"db = SQLDatabase.from_uri(db_uri) is giving me the error \"ValidationError","last_message_id":"1116752845673341089","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:35:59.725000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:35:59.725000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","161206345620652032"]}},{"id":"1116750223847465083","type":0,"content":"<@1072591948499664996> when I pass a callback manager to my tool, with an `on_chain_end` callback, it is being triggered by other chains that are not part of that tool. How can I prevent this?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:26:41.581000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116750223847465083","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when I pass a callback manager to my tool, with an `on_chain_end` callback, it is being tr","last_message_id":"1116753668608368812","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:26:41.849000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:26:41.849000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","359130672872423464"]}},{"id":"1116747967592923207","type":0,"content":"<@1072591948499664996> What is the best way to give a memory variable to router chain ?","channel_id":"1072944049788555314","author":{"id":"1006546349552062555","username":"saxenauts","global_name":null,"avatar":null,"discriminator":"0764","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:17:43.648000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116747967592923207","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What is the best way to give a memory variable to router chain ?","last_message_id":"1116757997755047947","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:17:43.852000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:17:43.852000+00:00"},"message_count":10,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":10,"member_ids_preview":["437808476106784770","1072591948499664996","1006546349552062555"]}},{"id":"1116745889264980008","type":0,"content":"<@1072591948499664996> When using ConversationalChatAgent, if I set early_stopping_method=\"generate\", I get the error: ValueError: variable agent_scratchpad should be a list of base messages","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:09:28.136000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116745889264980008","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"When using ConversationalChatAgent, if I set early_stopping_method=\"generate\", I get the e","last_message_id":"1116745987088728205","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:09:28.254000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:09:28.254000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["251035647689621504","1072591948499664996","437808476106784770"]}},{"id":"1116744804198526977","type":0,"content":"<@1072591948499664996> how can I use a callback_manager with a tool that's already been instantiated?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:05:09.436000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116744804198526977","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I use a callback_manager with a tool that's already been instantiated?","last_message_id":"1116744906665361450","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:05:09.918000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:05:09.918000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["359130672872423464","437808476106784770","1072591948499664996"]}},{"id":"1116744341642285218","type":0,"content":"<@1072591948499664996> how can I pass a callback manager to a tool after I create it using the @tool decorator?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T15:03:19.154000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116744341642285218","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I pass a callback manager to a tool after I create it using the @tool decorator?","last_message_id":"1116744422986625066","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T15:03:19.326000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T15:03:19.326000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["359130672872423464","437808476106784770","1072591948499664996"]}},{"id":"1116737140785684541","type":0,"content":"<@1072591948499664996> do you have document loaders specifically for code rather than text ?","channel_id":"1072944049788555314","author":{"id":"465814189747732490","username":"Media","global_name":null,"avatar":"556d87b1871b7d86b5fc8bf4352ac5ca","discriminator":"1029","public_flags":64,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T14:34:42.336000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116737140785684541","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"do you have document loaders specifically for code rather than text ?","last_message_id":"1116744231042695268","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T14:34:42.919000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T14:34:42.919000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["437808476106784770","1072591948499664996","465814189747732490"]}},{"id":"1116733463932960918","type":0,"content":"<@1072591948499664996> I don't understand the difference between chains and agents. Can you help explain when I would use one vs. the other when/how they can be used together?","channel_id":"1072944049788555314","author":{"id":"1058241473813938208","username":"Fima","global_name":null,"avatar":null,"discriminator":"1892","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T14:20:05.706000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116733463932960918","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I don't understand the difference between chains and agents. Can you help explain when I w","last_message_id":"1116869729747996803","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T14:20:05.871000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T14:20:05.871000+00:00"},"message_count":24,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":24,"member_ids_preview":["1058241473813938208","437808476106784770","1072591948499664996"]}},{"id":"1116728537311871180","type":0,"content":"<@1072591948499664996> how do i stream my LLM responses?","channel_id":"1072944049788555314","author":{"id":"516592645997592601","username":"huskywoof","global_name":null,"avatar":null,"discriminator":"5884","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T14:00:31.108000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116728537311871180","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i stream my LLM responses?","last_message_id":"1116732277557317715","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T14:00:31.337000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T14:00:31.337000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","516592645997592601","437808476106784770"]}},{"id":"1116718093713031188","type":0,"content":"<@1072591948499664996> how EmbeddingRouterChain works?","channel_id":"1072944049788555314","author":{"id":"1096457296273481790","username":"innovation_w","global_name":null,"avatar":null,"discriminator":"4349","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T13:19:01.160000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116718093713031188","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how EmbeddingRouterChain works?","last_message_id":"1116723857760456704","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T13:19:01.391000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T13:19:01.391000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1096457296273481790","1072591948499664996","437808476106784770"]}},{"id":"1116717587380830229","type":0,"content":"<@1072591948499664996> how ConversationChain works? from langchain.chains import ConversationChain","channel_id":"1072944049788555314","author":{"id":"1096457296273481790","username":"innovation_w","global_name":null,"avatar":null,"discriminator":"4349","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T13:17:00.441000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116717587380830229","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how ConversationChain works? from langchain.chains import ConversationChain","last_message_id":"1116717750455378082","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T13:17:00.619000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T13:17:00.619000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1096457296273481790","437808476106784770","1072591948499664996"]}},{"id":"1116715860233572463","type":0,"content":"<@1072591948499664996> how do I install detectron? I'm getting an error when running `python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'` runs into an error","channel_id":"1072944049788555314","author":{"id":"838372646294782002","username":"gopher","global_name":null,"avatar":"65c143e1c7a9bdff7ddbee0c265ba4ad","discriminator":"4086","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T13:10:08.657000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116715860233572463","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I install detectron? I'm getting an error when running `python -m pip install 'git+","last_message_id":"1116717725574762626","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T13:10:08.778000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T13:10:08.778000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","838372646294782002","437808476106784770"]}},{"id":"1116712822555033691","type":0,"content":"<@1072591948499664996>  have a chatbot that is powered by open ai api. It is designed to have a conversation with a user and ask them their name, phone, email, what business they are in, how big the business is etc. I store the bot messages and user responses in a table in supabase called user_response that has columns called bot_message and user_response respectively. I want to create code using langchain in typescript js, so that I can read the messages in the user_response table and parse the answers to the questions so that I have values for name, phone, email, business details, business size etc and insert them in another table that has the suitable columns for that. Note that user responses can be unpredictable as sometimes the users may choose not to answer a specific question or give an answer that does not make sense, so we need some sort of a mechanism that reads the messages and parses them using the open ai api somehow to sift out the appropriate data element. user responses do not follow any strict patterns and a regex based solution will not necessarily give predictable outcomes. Can you come up with an architecture that sends the messages to open ai api and then gets the api to spit out the elements by reading and analyzing the messages..Can you please assist with the code, make sure to provide detailed explanation.","channel_id":"1072944049788555314","author":{"id":"938959997604335676","username":"alpha","global_name":null,"avatar":"510f96d4778214c24bbd4e2c52637c2c","discriminator":"4769","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T12:58:04.418000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116712822555033691","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"have a chatbot that is powered by open ai api. It is designed to have a conversation with","last_message_id":"1116713177829343252","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T12:58:04.596000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T12:58:04.596000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","938959997604335676","1072591948499664996"]}},{"id":"1116712048227786832","type":0,"content":"<@1072591948499664996> i am trying to find an example of calling the openai completion.create in js. Can you provide me one.","channel_id":"1072944049788555314","author":{"id":"938959997604335676","username":"alpha","global_name":null,"avatar":"510f96d4778214c24bbd4e2c52637c2c","discriminator":"4769","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T12:54:59.804000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116712048227786832","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i am trying to find an example of calling the openai completion.create in js. Can you prov","last_message_id":"1116712177143914626","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T12:55:00.144000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T12:55:00.144000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["938959997604335676","1072591948499664996","437808476106784770"]}},{"id":"1116707297993953381","type":0,"content":"<@1072591948499664996> how to specify output variables when using a sequential chain composed of multiple LLM chains. I would like to know how to specify the ouput variables of individual LLM Chains","channel_id":"1072944049788555314","author":{"id":"76841662445256704","username":"equinoxd","global_name":"Equinox","avatar":"ed5330cee29e6796621dd1f7c56a3787","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T12:36:07.260000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116707297993953381","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to specify output variables when using a sequential chain composed of multiple LLM cha","last_message_id":"1116707497915449344","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T12:36:07.444000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T12:36:07.444000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","76841662445256704","1072591948499664996"]}},{"id":"1116705326834647100","type":0,"content":"<@1072591948499664996> In Langchain python, when using a OpenAIChat model, is the chat history saved when I run a LLMChain or is there another type of chain I need to run so that everytime I add a new prompt the previous system & user messages are kept ?","channel_id":"1072944049788555314","author":{"id":"76841662445256704","username":"equinoxd","global_name":"Equinox","avatar":"ed5330cee29e6796621dd1f7c56a3787","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T12:28:17.299000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116705326834647100","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"In Langchain python, when using a OpenAIChat model, is the chat history saved when I run a","last_message_id":"1116706430960029747","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T12:28:17.474000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T12:28:17.474000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","76841662445256704","437808476106784770"]}},{"id":"1116701348046581831","type":0,"content":"<@1072591948499664996> is it possible to use qdrant with in memory vector store? Or do I have to use the cloud? I want to host a langchain script on streamlit and read in a lot of documents then pre select them via qdrant","channel_id":"1072944049788555314","author":{"id":"637204983338500096","username":"habeck","global_name":null,"avatar":null,"discriminator":"7117","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T12:12:28.682000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116701348046581831","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"is it possible to use qdrant with in memory vector store? Or do I have to use the cloud? I","last_message_id":"1116701513268592640","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T12:12:28.891000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T12:12:28.891000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","637204983338500096","1072591948499664996"]}},{"id":"1116696872761966602","type":0,"content":"<@1072591948499664996>  I have the folllowing Prompt: \n\n`PROMPT_TEMPLATE = \"\"\"Use the following pieces of context and past chat history to answer the question at the end. \nIf you don't know the answer, just say, \"I don't know. Out of Context provided\". Don't try to make up an answer.\nAnswer only for questions within the context provided. Answer in language asked. \n\n{context}\n\n\nQuestion: {question}\nAnswer in {language}:\"\"\"`\n\n\nI want it to be passed via RetrievalQA chain so that I can fetch the right answer from the doc. How do I add the above prompt to a RetrievalQA chain","channel_id":"1072944049788555314","author":{"id":"780667203967254529","username":"btisback","global_name":null,"avatar":"c1140d00a052cf98d2ccd22cdc5ad6bf","discriminator":"5898","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T11:54:41.691000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116696872761966602","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have the folllowing Prompt: `PROMPT_TEMPLATE = \"\"\"Use the following pieces of context","last_message_id":"1116698275161055322","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T11:54:41.810000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T11:54:41.810000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["437808476106784770","780667203967254529","1072591948499664996"]}},{"id":"1116674041797939240","type":0,"content":"<@1072591948499664996>  is there any way to batch multiple prompts for inference and sending it in parallel through a single api call and collect all the responses together","channel_id":"1072944049788555314","author":{"id":"740873856079626291","username":"DeadShot609","global_name":null,"avatar":"9310f98e47e0edfc8e6494f99b0a939f","discriminator":"0048","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T10:23:58.365000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116674041797939240","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"is there any way to batch multiple prompts for inference and sending it in parallel throug","last_message_id":"1116692975016214638","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T10:23:58.502000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T10:23:58.502000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["437808476106784770","740873856079626291","1072591948499664996"]}},{"id":"1116661440649113661","type":0,"content":"<@1072591948499664996> why is serpapi not giving me current data","channel_id":"1072944049788555314","author":{"id":"358935632682156033","username":"DoubleAs","global_name":null,"avatar":"e2b7efbcef7e12f64f698865816bacad","discriminator":"0437","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T09:33:54.017000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116661440649113661","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"why is serpapi not giving me current data","last_message_id":"1116661535150964746","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T09:33:54.175000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T09:33:54.175000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","358935632682156033"]}},{"id":"1116657227525259304","type":0,"content":"<@1072591948499664996> what's the best workflow to generate a response to a prompt based on provided documents, on one side, and the conversation history, on the other side?","channel_id":"1072944049788555314","author":{"id":"128604800156696576","username":"evenfrost","global_name":"Evenfrost","avatar":"5295f4fbc2c840de9438c21db2407573","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T09:17:09.530000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116657227525259304","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what's the best workflow to generate a response to a prompt based on provided documents, o","last_message_id":"1116703747368816760","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T09:17:09.691000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T09:17:09.691000+00:00"},"message_count":19,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":19,"member_ids_preview":["128604800156696576","437808476106784770","1072591948499664996"]}},{"id":"1116649547108712449","type":0,"content":"<@1072591948499664996> how can i specifiy how many chucks i want to input into my prompt","channel_id":"1072944049788555314","author":{"id":"358935632682156033","username":"DoubleAs","global_name":null,"avatar":"e2b7efbcef7e12f64f698865816bacad","discriminator":"0437","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T08:46:38.376000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116649547108712449","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i specifiy how many chucks i want to input into my prompt","last_message_id":"1116649634593505341","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T08:46:38.507000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T08:46:38.507000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","358935632682156033"]}},{"id":"1116636019358449704","type":0,"content":"<@1072591948499664996> what is the difference between MapReduceDocumentsChain and loadQAMapReduceChain","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T07:52:53.109000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116636019358449704","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between MapReduceDocumentsChain and loadQAMapReduceChain","last_message_id":"1116636263731183617","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T07:52:53.237000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T07:52:53.237000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1107656108199395328","1072591948499664996","437808476106784770"]}},{"id":"1116635647441121290","type":0,"content":"<@1072591948499664996> Can I use a self-query retriever in a qa chain?","channel_id":"1072944049788555314","author":{"id":"821772633422954527","username":"Tommaso De Lorenzo","global_name":null,"avatar":"af1144c80f484b055e41ebfe83ec37f9","discriminator":"4447","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T07:51:24.437000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116635647441121290","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Can I use a self-query retriever in a qa chain?","last_message_id":"1116635977562198086","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T07:51:24.591000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T07:51:24.591000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","821772633422954527","1072591948499664996"]}},{"id":"1116631709190996048","type":0,"content":"<@1072591948499664996> how to add #panda\ndf = pd.read_csv('ourdata.csv')\nagent = create_pandas_dataframe_agent(OpenAI(temperature=0), df, verbose=True)\nto a  simpleChain ?","channel_id":"1072944049788555314","author":{"id":"897492138416300052","username":"ElDiablo","global_name":"Admin","avatar":"c27d324215a2d0b1fb56f858a9e6621d","discriminator":"5947","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T07:35:45.485000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116631709190996048","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to add #pandadf = pd.read_csv('ourdata.csv')agent = create_pandas_dataframe_agent(Op","last_message_id":"1116631811959820419","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T07:35:45.669000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T07:35:45.669000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","897492138416300052","437808476106784770"]}},{"id":"1116630100973211669","type":0,"content":"<@1072591948499664996> are there any recommended chatbot react packages?","channel_id":"1072944049788555314","author":{"id":"374902921537781763","username":"Michael","global_name":null,"avatar":"fab402c188d384a1893dc49f84963f4c","discriminator":"0331","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T07:29:22.056000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116630100973211669","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"are there any recommended chatbot react packages?","last_message_id":"1116630227460816906","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T07:29:22.171000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T07:29:22.171000+00:00"},"message_count":3,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770"]}},{"id":"1116626226950455328","type":0,"content":"<@1072591948499664996> How do I apply map reduce chain in javascript","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T07:13:58.417000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116626226950455328","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I apply map reduce chain in javascript","last_message_id":"1116626435411562506","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T07:13:58.677000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T07:13:58.677000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","1107656108199395328"]}},{"id":"1116619157904031764","type":0,"content":"<@1072591948499664996> how to put conversationhistory in multipromptchain","channel_id":"1072944049788555314","author":{"id":"690535777809072195","username":"nazhan","global_name":null,"avatar":"b145069903da09988cd96eb751775cb3","discriminator":"4297","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T06:45:53.025000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116619157904031764","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to put conversationhistory in multipromptchain","last_message_id":"1116619255232864357","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T06:45:53.146000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T06:45:53.146000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["690535777809072195","1072591948499664996","437808476106784770"]}},{"id":"1116614269300125776","type":0,"content":"<@1072591948499664996> whats the difference between HuggingFaceEmbeddings and HuggingFaceHubEmbeddings?","channel_id":"1072944049788555314","author":{"id":"392717162797596672","username":"Homosexual Toaster","global_name":null,"avatar":"9ed8b3e96e10aee283a664c112fd368c","discriminator":"6083","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T06:26:27.491000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116614269300125776","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"whats the difference between HuggingFaceEmbeddings and HuggingFaceHubEmbeddings?","last_message_id":"1116621159774027776","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T06:26:27.667000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T06:26:27.667000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["392717162797596672","1072591948499664996","437808476106784770"]}},{"id":"1116612619407724634","type":0,"content":"<@1072591948499664996> what is the difference between OpenAI and ChatOpenAI ?","channel_id":"1072944049788555314","author":{"id":"378791112736899073","username":"0xnuts","global_name":"0xnuts","avatar":"d2050091b956250174e2f8f6633c4f3a","discriminator":"1004","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T06:19:54.126000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116612619407724634","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between OpenAI and ChatOpenAI ?","last_message_id":"1116613139803414618","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T06:19:54.257000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T06:19:54.257000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","378791112736899073"]}},{"id":"1116607040152936518","type":0,"content":"<@1072591948499664996> trying to use VectorstoreIndexCreator, but i feel it keeps it might re-embed documents that were already persisted.","channel_id":"1072944049788555314","author":{"id":"202585503466258432","username":"crazysim","global_name":"crazysim","avatar":"af9af21039fa36207d247df1b344e9be","discriminator":"0","public_flags":0,"avatar_decoration":"v3_a_88765df09a036906b376704bd8dc27ba"},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T05:57:43.928000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116607040152936518","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"trying to use VectorstoreIndexCreator, but i feel it keeps it might re-embed documents tha","last_message_id":"1116607182490832906","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T05:57:44.111000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T05:57:44.111000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","202585503466258432"]}},{"id":"1116605755693482074","type":0,"content":"<@1072591948499664996> I am using serpapi to search the web and send me urls, but the urls it returns are invalid. How can I ensure it returns valid urls (Please send me a list of ALL possible solutions)","channel_id":"1072944049788555314","author":{"id":"466055658765025280","username":"haikdc","global_name":null,"avatar":null,"discriminator":"5614","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T05:52:37.689000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116605755693482074","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am using serpapi to search the web and send me urls, but the urls it returns are invalid","last_message_id":"1116605853785669632","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T05:52:37.824000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T05:52:37.824000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["466055658765025280","1072591948499664996","437808476106784770"]}},{"id":"1116604063430213672","type":0,"content":"<@1072591948499664996> how do I use the llama weights from Meta to create a custom llm wrapper?","channel_id":"1072944049788555314","author":{"id":"392717162797596672","username":"Homosexual Toaster","global_name":null,"avatar":"9ed8b3e96e10aee283a664c112fd368c","discriminator":"6083","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T05:45:54.222000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116604063430213672","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I use the llama weights from Meta to create a custom llm wrapper?","last_message_id":"1116605075050217573","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T05:45:54.489000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T05:45:54.489000+00:00"},"message_count":8,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["392717162797596672","1072591948499664996"]}},{"id":"1116603686047731742","type":0,"content":"<@1072591948499664996> how do i prevent URL hallucinations?","channel_id":"1072944049788555314","author":{"id":"378791112736899073","username":"0xnuts","global_name":"0xnuts","avatar":"d2050091b956250174e2f8f6633c4f3a","discriminator":"1004","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T05:44:24.247000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116603686047731742","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i prevent URL hallucinations?","last_message_id":"1116603746445692978","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T05:44:24.494000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T05:44:24.494000+00:00"},"message_count":3,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","378791112736899073"]}},{"id":"1116602558589448233","type":0,"content":"<@1072591948499664996> in following js code:\n```\n    let retriever = this.documents.get(id).vectorStore.asRetriever();\n    const results = await retriever.getRelevantDocuments(query);\n```\nhow can I config search params? like k number?","channel_id":"1072944049788555314","author":{"id":"1069844052134080542","username":"carlay","global_name":null,"avatar":"d586b37962d1617b2a1017049d181b96","discriminator":"8826","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T05:39:55.440000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116602558589448233","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"in following js code:```    let retriever = this.documents.get(id).vectorStore.asRetriev","last_message_id":"1116602690781323264","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T05:39:55.598000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T05:39:55.598000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","1069844052134080542","437808476106784770"]}},{"id":"1116590015615606915","type":0,"content":"<@1072591948499664996> use this to make a call to chatopen ai\n\nSYSTEM_PROMPT = \"\"\"\n// You are WolframLanguageConverterGPT. Whenver given a text input you output only syntactically correct Wolfram Language code. Here are your guidelines\n// - ALWAYS use this exponent notation: 6*10^14, NEVER 6e14.\n// - ALWAYS use proper Markdown formatting for all math, scientific, and chemical formulas, symbols, etc.:  '$$\\n[expression]\\n$$' for standalone cases and '\\( [expression] \\)' when inline.\n\"\"\"\n\nHUMAN_PROMPT = \"\"\" Can you please convert the follwoing to Wolfram language code {problem}\"\"\"","channel_id":"1072944049788555314","author":{"id":"686330448204529704","username":"jayr1234","global_name":null,"avatar":"b15a7c63146e07bd7178e949dd051182","discriminator":"7481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T04:50:04.962000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116590015615606915","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"use this to make a call to chatopen aiSYSTEM_PROMPT = \"\"\"// You are WolframLanguageConv","last_message_id":"1116590141436342323","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T04:50:05.102000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T04:50:05.102000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","686330448204529704"]}},{"id":"1116581725875748925","type":0,"content":"<@1072591948499664996> How can I use a lora with LlamaCPP in langchain?","channel_id":"1072944049788555314","author":{"id":"229576111212855296","username":"coffeevampir3","global_name":"Coffee Vampire","avatar":"303d86ba731d14668c4b46b6dae38ef9","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T04:17:08.534000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116581725875748925","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I use a lora with LlamaCPP in langchain?","last_message_id":"1116581858646429777","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T04:17:08.734000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T04:17:08.734000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["229576111212855296","1072591948499664996","437808476106784770"]}},{"id":"1116574499144470558","type":0,"content":"<@1072591948499664996> what is LLMChain","channel_id":"1072944049788555314","author":{"id":"686330448204529704","username":"jayr1234","global_name":null,"avatar":"b15a7c63146e07bd7178e949dd051182","discriminator":"7481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:48:25.547000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116574499144470558","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is LLMChain","last_message_id":"1116580613990916138","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:48:25.712000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:48:25.712000+00:00"},"message_count":19,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":19,"member_ids_preview":["1072591948499664996","437808476106784770","686330448204529704"]}},{"id":"1116571524745404445","type":0,"content":"<@1072591948499664996> I have a system prompt and a user prompt I want to send to the chat completion endpoint. How can I turn this into a tool in python","channel_id":"1072944049788555314","author":{"id":"686330448204529704","username":"jayr1234","global_name":null,"avatar":"b15a7c63146e07bd7178e949dd051182","discriminator":"7481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:36:36.395000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116571524745404445","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have a system prompt and a user prompt I want to send to the chat completion endpoint. H","last_message_id":"1116577398113190008","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:36:36.581000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:36:36.581000+00:00"},"message_count":16,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":16,"member_ids_preview":["437808476106784770","1072591948499664996","686330448204529704"]}},{"id":"1116571104895582218","type":0,"content":"<@&1072943855747481672> How can I load LORA's for the language models?","channel_id":"1072944049788555314","author":{"id":"229576111212855296","username":"coffeevampir3","global_name":"Coffee Vampire","avatar":"303d86ba731d14668c4b46b6dae38ef9","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:34:56.295000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116568310025306163","type":0,"content":"<@1072591948499664996> salesGPT„ÅÆconbersation_type„Å´„ÅØ„Å©„Çì„Å™Á®ÆÈ°û„Åå„ÅÇ„Çä„Åæ„Åô„Åã","channel_id":"1072944049788555314","author":{"id":"1092624100998004838","username":"RY","global_name":null,"avatar":"4c1cf5f618b2b760ebeb6977c17183e7","discriminator":"4944","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:23:49.946000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116568310025306163","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"salesGPT„ÅÆconbersation_type„Å´„ÅØ„Å©„Çì„Å™Á®ÆÈ°û„Åå„ÅÇ„Çä„Åæ„Åô„Åã","last_message_id":"1116651773071999016","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:23:50.108000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:23:50.108000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","1092624100998004838","437808476106784770"]}},{"id":"1116567648419983361","type":0,"content":"<@1072591948499664996> how do I do sub queries? That takes an input question, breaks it down into sub/smaller questions and returns an aggregate answer","channel_id":"1072944049788555314","author":{"id":"344917911372169227","username":"Batman","global_name":null,"avatar":"092da3040e48f1f60898eea8f498fae6","discriminator":"7634","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:21:12.207000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116567648419983361","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I do sub queries? That takes an input question, breaks it down into sub/smaller que","last_message_id":"1116567849603969055","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:21:12.881000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:21:12.881000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","344917911372169227","1072591948499664996"]}},{"id":"1116564628617580544","type":0,"content":"<@1072591948499664996> can I define a tool that returns something other than just text to the agent chain?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:09:12.230000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116564628617580544","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"can I define a tool that returns something other than just text to the agent chain?","last_message_id":"1116564785727819826","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:09:13.025000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:09:13.025000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["359130672872423464","437808476106784770","1072591948499664996"]}},{"id":"1116562940267610192","type":0,"content":"<@1072591948499664996>  What Custom Parameters apply to SerpAPI, the docs say you can use arbitrary parameters but I don't know what this means","channel_id":"1072944049788555314","author":{"id":"466055658765025280","username":"haikdc","global_name":null,"avatar":null,"discriminator":"5614","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T03:02:29.696000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116562940267610192","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"What Custom Parameters apply to SerpAPI, the docs say you can use arbitrary parameters but","last_message_id":"1116563175094095892","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T03:02:29.814000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T03:02:29.814000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","466055658765025280","437808476106784770"]}},{"id":"1116554001945743390","type":0,"content":"<@1072591948499664996> Can I use two different llms (or different llm parameters) in load_qa_chain (map reduce)?","channel_id":"1072944049788555314","author":{"id":"410618421923807242","username":"derekhsu","global_name":null,"avatar":"404e9728fa077fa146b902bb1d25f2cf","discriminator":"4584","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T02:26:58.634000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116554001945743390","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Can I use two different llms (or different llm parameters) in load_qa_chain (map reduce)?","last_message_id":"1116555013376978995","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T02:26:58.781000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T02:26:58.781000+00:00"},"message_count":2,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":2,"member_ids_preview":["1072591948499664996","410618421923807242","437808476106784770"]}},{"id":"1116548259285057607","type":0,"content":"<@1072591948499664996> I want to create a Chroma vectorstore that can have new documents added dynamically. This means I can add a pdf, query the model, then add another pdf. How would I design something like this?","channel_id":"1072944049788555314","author":{"id":"231174216693710848","username":"thedeadmanwalking","global_name":"Deadman","avatar":"c0cf4d4779b64c0621542a8b89e9f579","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T02:04:09.477000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116548259285057607","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I want to create a Chroma vectorstore that can have new documents added dynamically. This","last_message_id":"1116548636277473281","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T02:04:09.642000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T02:04:09.642000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","231174216693710848"]}},{"id":"1116545025719930930","type":0,"content":"<@1072591948499664996> Invalid prompt schema; check for mismatched or missing input parameters. {'input', 'agent_scratchpad'} (type=value_error)","channel_id":"1072944049788555314","author":{"id":"1092624100998004838","username":"RY","global_name":null,"avatar":"4c1cf5f618b2b760ebeb6977c17183e7","discriminator":"4944","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T01:51:18.535000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116545025719930930","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Invalid prompt schema; check for mismatched or missing input parameters. {'input', 'agent_","last_message_id":"1116568261912436766","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T01:51:18.761000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T01:51:18.761000+00:00"},"message_count":22,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":22,"member_ids_preview":["437808476106784770","1092624100998004838","1072591948499664996"]}},{"id":"1116540052248469504","type":0,"content":"<@1072591948499664996> Agent„Å®Ëá™Ââç„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰Ωø„Å£„Åü‰æã„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ","channel_id":"1072944049788555314","author":{"id":"1092624100998004838","username":"RY","global_name":null,"avatar":"4c1cf5f618b2b760ebeb6977c17183e7","discriminator":"4944","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T01:31:32.767000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116540052248469504","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Agent„Å®Ëá™Ââç„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰Ωø„Å£„Åü‰æã„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ","last_message_id":"1116544984397664277","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T01:31:32.963000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T01:31:32.963000+00:00"},"message_count":23,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":23,"member_ids_preview":["1072591948499664996","437808476106784770","1092624100998004838"]}},{"id":"1116534215052382290","type":0,"content":"<@1072591948499664996> I would like to build a chatbot that has memory. How can I do that?","channel_id":"1072944049788555314","author":{"id":"413370296511234048","username":"ShaunO","global_name":null,"avatar":"e3fe777664a6ba1b7061e6fdd9a78252","discriminator":"8862","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-09T01:08:21.071000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116534215052382290","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I would like to build a chatbot that has memory. How can I do that?","last_message_id":"1116599168560013363","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-09T01:08:21.387000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-09T01:08:21.387000+00:00"},"message_count":13,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":13,"member_ids_preview":["413370296511234048","1072591948499664996","437808476106784770"]}},{"id":"1116497624086884423","type":0,"content":"<@1072591948499664996> I am using load_qa_with_sources_chain. How can I format the SOURCES part of the output?","channel_id":"1072944049788555314","author":{"id":"1108877422583107645","username":"rhruby","global_name":null,"avatar":null,"discriminator":"4189","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:42:57.105000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116497624086884423","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am using load_qa_with_sources_chain. How can I format the SOURCES part of the output?","last_message_id":"1116497790915317792","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:42:57.338000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:42:57.338000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1108877422583107645","437808476106784770","1072591948499664996"]}},{"id":"1116496233633165322","type":0,"content":"<@&1072943855747481672> How do the inputs to the prompttemplate differ? What determines what variables can be used in python","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:37:25.595000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116495135040090163","type":0,"content":"<@1072591948499664996>  how can I track token and call usage in Javascript in Langchain","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:33:03.670000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116495135040090163","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I track token and call usage in Javascript in Langchain","last_message_id":"1116496661645103224","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:33:03.813000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:33:03.813000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":14,"member_ids_preview":["1072591948499664996","1107656108199395328","437808476106784770"]}},{"id":"1116494845016551539","type":0,"content":"<@1072591948499664996> explain to me in terms of prompttemplates and memory the difference between chains and agents","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:31:54.523000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116494845016551539","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"explain to me in terms of prompttemplates and memory the difference between chains and age","last_message_id":"1116495951876608121","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:31:54.756000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:31:54.756000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","719286426063339571","437808476106784770"]}},{"id":"1116493479988690944","type":0,"content":"<@1072591948499664996> when using an agent, how to stream only the output through the \"on_llm_new_token\" callback method?","channel_id":"1072944049788555314","author":{"id":"757924024520015882","username":"Demba","global_name":null,"avatar":null,"discriminator":"0871","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:26:29.075000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116493479988690944","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when using an agent, how to stream only the output through the \"on_llm_new_token\" callback","last_message_id":"1116493721047933028","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:26:29.265000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:26:29.265000+00:00"},"message_count":2,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":2,"member_ids_preview":["1072591948499664996","437808476106784770","757924024520015882"]}},{"id":"1116493049506300026","type":0,"content":"<@1072591948499664996> When using an agent, is it possible to stream only the output?","channel_id":"1072944049788555314","author":{"id":"757924024520015882","username":"Demba","global_name":null,"avatar":null,"discriminator":"0871","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:24:46.440000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116493049506300026","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"When using an agent, is it possible to stream only the output?","last_message_id":"1116493146545733632","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:24:46.603000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:24:46.603000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","757924024520015882","1072591948499664996"]}},{"id":"1116491431134105763","type":0,"content":"<@1072591948499664996> how can i persist a ConversationEntityMemory memory store? I want to save it somewhere","channel_id":"1072944049788555314","author":{"id":"719286426063339571","username":"trouble_","global_name":null,"avatar":"94dc5e5945326bed94b71db1fc9cb3b2","discriminator":"4085","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:18:20.590000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116491431134105763","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i persist a ConversationEntityMemory memory store? I want to save it somewhere","last_message_id":"1116492214709800980","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:18:20.780000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:18:20.780000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","719286426063339571"]}},{"id":"1116486893522133022","type":0,"content":"<@1072591948499664996> Here is a tool instantiated with an @tool decorator: \n```\n    @tool(\"vector_db_qa_tool\")\n    def vector_db_qa_tool(question: str) -> Tuple[str, List[str]]:\n        \"\"\"A tool for retrieving answers using the vector_db_qa_chain.\"\"\"\n        result = vector_db_qa_chain({\"question\": question})\n\n```\n\nHow do I create the equivalent tool by subclassing the `BaseTool` rather than by using the decorator?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T22:00:18.739000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116486893522133022","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Here is a tool instantiated with an @tool decorator: ```    @tool(\"vector_db_qa_tool\")","last_message_id":"1116493348266573908","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T22:00:18.883000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T22:00:18.883000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","437808476106784770","359130672872423464"]}},{"id":"1116483741729165395","type":0,"content":"<@1072591948499664996> how do I pass a non-standard output parser to a `RetrievalQAWithSourcesChain`?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:47:47.293000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116483741729165395","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I pass a non-standard output parser to a `RetrievalQAWithSourcesChain`?","last_message_id":"1116484020268695652","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:47:47.438000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:47:47.438000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","359130672872423464","437808476106784770"]}},{"id":"1116482414953369601","type":0,"content":"<@1072591948499664996> what is the default tool output parser? I want to parse the output of my tool before it reaches the agent's main chain.","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:42:30.965000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116482414953369601","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the default tool output parser? I want to parse the output of my tool before it re","last_message_id":"1116482792356843531","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:42:31.206000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:42:31.206000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","359130672872423464"]}},{"id":"1116481559332126791","type":0,"content":"<@1072591948499664996> how can i track token usage in js","channel_id":"1072944049788555314","author":{"id":"1074472737940189266","username":"gullerg","global_name":null,"avatar":null,"discriminator":"4494","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:39:06.969000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116481559332126791","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i track token usage in js","last_message_id":"1116481707273629820","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:39:07.121000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:39:07.121000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1074472737940189266","437808476106784770","1072591948499664996"]}},{"id":"1116480207369543800","type":0,"content":"<@1072591948499664996> how can i track token usage in js","channel_id":"1072944049788555314","author":{"id":"1074472737940189266","username":"gullerg","global_name":null,"avatar":null,"discriminator":"4494","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:33:44.636000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116480207369543800","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i track token usage in js","last_message_id":"1116481446475989083","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:33:44.778000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:33:44.778000+00:00"},"message_count":6,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":6,"member_ids_preview":["1072591948499664996","437808476106784770","1074472737940189266"]}},{"id":"1116479227617878117","type":0,"content":"<@1072591948499664996> how do I use RetrievalQAWithSourcesChain?","channel_id":"1072944049788555314","author":{"id":"1012449655298215987","username":"JDT","global_name":null,"avatar":"8248e001464a7fce7ef8974dddc9d363","discriminator":"8299","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:29:51.045000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116479227617878117","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I use RetrievalQAWithSourcesChain?","last_message_id":"1116479550549934140","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:29:51.209000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:29:51.209000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","1012449655298215987"]}},{"id":"1116477123822100531","type":0,"content":"<@1072591948499664996>  I have a tool that is being run by an agent. However, if I reference something that was observed as an output by this tool in an intermediate step, the agent attempts to rerun the whole sequence to get data rather than referencing the observation from the last execution. How can I store or reference specific outputs from tools?","channel_id":"1072944049788555314","author":{"id":"359842104798871563","username":"yungseneca","global_name":null,"avatar":"3d049fc03743b85f529dfe952acde880","discriminator":"8481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T21:21:29.461000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116477123822100531","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have a tool that is being run by an agent. However, if I reference something that was ob","last_message_id":"1116478297640009738","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T21:21:29.610000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T21:21:29.610000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","437808476106784770","359842104798871563"]}},{"id":"1116469442247917659","type":0,"content":"<@1072591948499664996> I have several discord channels scraped and put into several csv file. How would I create a chat bot that can answer questions based on this scraped data","channel_id":"1072944049788555314","author":{"id":"140594162050400256","username":"quickandsmart","global_name":"QuickAndSmart","avatar":"5a4933b2a4b09df6893ddbac79ba3e9c","discriminator":"0","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T20:50:58.031000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116469442247917659","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have several discord channels scraped and put into several csv file. How would I create","last_message_id":"1116770674447482880","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T20:50:58.275000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T20:50:58.275000+00:00"},"message_count":22,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":23,"member_ids_preview":["140594162050400256","1072591948499664996","437808476106784770"]}},{"id":"1116461945806131271","type":0,"content":"<@1072591948499664996> what is the difference between ChatOpenAI and OpenAI LLMs?","channel_id":"1072944049788555314","author":{"id":"1075581942361227294","username":"rlm","global_name":null,"avatar":null,"discriminator":"7117","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T20:21:10.740000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116461945806131271","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between ChatOpenAI and OpenAI LLMs?","last_message_id":"1116482076280107061","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T20:21:11.021000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T20:21:11.021000+00:00"},"message_count":16,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":16,"member_ids_preview":["1072591948499664996","437808476106784770","1075581942361227294"]}},{"id":"1116458719220346891","type":0,"content":"<@1072591948499664996> I created a vector for GCP Matching Engine using the following code:\n\nfrom langchain.embeddings import VertexAIEmbeddings\nembeddings = VertexAIEmbeddings()\nvector_store = MatchingEngine.from_components(\n                    index_id=INDEX,\n                    region=REGION,\n                    embedding=embeddings,\n                    project_id=PROJECT_ID,\n                    endpoint_id=ENDPOINT,\n                    gcs_bucket_name=DOCS_BUCKET\n                    )\nvector_store.similarity_search(\"what is life?\", k=8)\n\nthe error is: \"UNKNOWN:DNS resolution failed for :10000: unparseable host:port\". any idea what can be going on?","channel_id":"1072944049788555314","author":{"id":"778054458901921823","username":"pablo_filippi","global_name":"pol","avatar":null,"discriminator":"4879","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T20:08:21.462000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116458719220346891","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I created a vector for GCP Matching Engine using the following code:from langchain.embed","last_message_id":"1116458873973395576","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T20:08:21.802000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T20:08:21.802000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","778054458901921823"]}},{"id":"1116457535310938154","type":0,"content":"<@1072591948499664996> I created a vector store for GCP Matching Engine and when trying to do a similarity search, I get error \"UNKNOWN:DNS resolution failed for :10000: unparseable host:port\"","channel_id":"1072944049788555314","author":{"id":"778054458901921823","username":"pablo_filippi","global_name":"pol","avatar":null,"discriminator":"4879","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T20:03:39.196000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116457535310938154","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I created a vector store for GCP Matching Engine and when trying to do a similarity search","last_message_id":"1116457682317103104","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T20:03:39.393000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T20:03:39.393000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","778054458901921823"]}},{"id":"1116456522155819008","type":0,"content":"<@1072591948499664996> how do I create a custom Document object in Langchain?","channel_id":"1072944049788555314","author":{"id":"915229846592946237","username":"Niksta -------->>> Let‚Äôs go! üî•","global_name":null,"avatar":"421a127232684dfcfe183a2152384481","discriminator":"5365","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:59:37.641000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116456522155819008","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I create a custom Document object in Langchain?","last_message_id":"1116461472671858900","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:59:37.869000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:59:37.869000+00:00"},"message_count":19,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":19,"member_ids_preview":["437808476106784770","915229846592946237","1072591948499664996"]}},{"id":"1116454220762185879","type":0,"content":"<@1072591948499664996> how do I view how many tokens were used in javascript using langchain and pinecone?","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:50:28.946000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116454220762185879","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I view how many tokens were used in javascript using langchain and pinecone?","last_message_id":"1116454969420300339","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:50:29.231000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:50:29.231000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","1072591948499664996","1107656108199395328"]}},{"id":"1116453501065756762","type":0,"content":"<@1072591948499664996> My agent fails to follow my the Thought, Action pattern with gpt-3.5-turbo which results if parsing errors. Any tips on making that work?","channel_id":"1072944049788555314","author":{"id":"344575315592609792","username":"Thorge","global_name":null,"avatar":"dd3c82816a916760f8a9e4749a4cc6b6","discriminator":"2014","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:47:37.357000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116453501065756762","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"My agent fails to follow my the Thought, Action pattern with gpt-3.5-turbo which results i","last_message_id":"1116453844969328690","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:47:37.512000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:47:37.512000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","344575315592609792","437808476106784770"]}},{"id":"1116451235617329294","type":0,"content":"<@1072591948499664996> in javascript how do I use metadata filters","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:38:37.232000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116451235617329294","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"in javascript how do I use metadata filters","last_message_id":"1116451848719716473","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:38:37.401000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:38:37.401000+00:00"},"message_count":5,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":5,"member_ids_preview":["437808476106784770","1107656108199395328","1072591948499664996"]}},{"id":"1116449974503342210","type":0,"content":"<@1072591948499664996> when using the on_llm_new_token callback method, what other properties are accessible apart from the generated token itself?","channel_id":"1072944049788555314","author":{"id":"757924024520015882","username":"Demba","global_name":null,"avatar":null,"discriminator":"0871","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:33:36.559000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116449974503342210","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when using the on_llm_new_token callback method, what other properties are accessible apar","last_message_id":"1116450190468063282","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:33:36.904000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:33:36.904000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","757924024520015882","1072591948499664996"]}},{"id":"1116444303573667911","type":0,"content":"<@1072591948499664996> How do I add custom metadata tags to my loaded PDFs?","channel_id":"1072944049788555314","author":{"id":"1107656108199395328","username":"dagomes","global_name":"dgms","avatar":null,"discriminator":"5078","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:11:04.504000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116444303573667911","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I add custom metadata tags to my loaded PDFs?","last_message_id":"1116444483672879135","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:11:04.664000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:11:04.664000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","1107656108199395328"]}},{"id":"1116443701875593336","type":0,"content":"<@&1072943855747481672> How to put output parser in a python custom tool?","channel_id":"1072944049788555314","author":{"id":"359842104798871563","username":"yungseneca","global_name":null,"avatar":"3d049fc03743b85f529dfe952acde880","discriminator":"8481","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:08:41.048000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116443322303643738","type":0,"content":"<@1072591948499664996> can a tool description recieve a string with a variable inside?","channel_id":"1072944049788555314","author":{"id":"989617077021196372","username":"Erickson Siqueira","global_name":null,"avatar":"b34a26acff80578dcd618a81f0676861","discriminator":"2372","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T19:07:10.551000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116443322303643738","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"can a tool description recieve a string with a variable inside?","last_message_id":"1116451190029422683","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T19:07:10.677000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T19:07:10.677000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["989617077021196372","1072591948499664996","437808476106784770"]}},{"id":"1116438684108062731","type":0,"content":"<@1072591948499664996> How do I set up a conversational chatbot that can query postgres database?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:48:44.719000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116438684108062731","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I set up a conversational chatbot that can query postgres database?","last_message_id":"1116453204918538352","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:48:44.861000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:48:44.861000+00:00"},"message_count":20,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":20,"member_ids_preview":["437808476106784770","161206345620652032","1072591948499664996"]}},{"id":"1116438183543058492","type":0,"content":"<@1072591948499664996> how do I get embeddings for text using chatgpt and compare the embedding to other embeddings","channel_id":"1072944049788555314","author":{"id":"400746939232026625","username":"Fj00 üá¶üá∂","global_name":null,"avatar":"5d67e2793b39c36e43b418c05fff5e2d","discriminator":"2139","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:46:45.375000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116438183543058492","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I get embeddings for text using chatgpt and compare the embedding to other embeddin","last_message_id":"1116438364925722694","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:46:45.499000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:46:45.499000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","400746939232026625"]}},{"id":"1116431743008899253","type":0,"content":"<@1072591948499664996> \nI am trying to pass an argument to the callback function when using a custom agent through an agent executor, so that it can be used with the on_llm_start callback  method.\n\nThe class is defined as follow:\nclass MyCustomHandlerOne(BaseCallbackHandler):\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> Any:\n        # Use the argument passed in kwargs\n        custom_arg = kwargs.get(\"custom_arg\")\n        print(f\"on_llm_start {serialized['name']} with custom_arg: {custom_arg}\")\n\n\nWhen I run the agent, I passed the custom argument in the callbacks keyword argument:\n\nagent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1], custom_arg=\"your_value\")\n\nHowever, I get the following error:\n\n\"One input key expected got ['custom_arg', 'input']\"","channel_id":"1072944049788555314","author":{"id":"757924024520015882","username":"Demba","global_name":null,"avatar":null,"discriminator":"0871","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:21:09.832000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116431743008899253","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am trying to pass an argument to the callback function when using a custom agent through","last_message_id":"1116431900697952307","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:21:09.975000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:21:09.975000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["757924024520015882","1072591948499664996","437808476106784770"]}},{"id":"1116430882086060082","type":0,"content":"<@1072591948499664996> How do I get started?","channel_id":"1072944049788555314","author":{"id":"1072507804511899678","username":"FinnKapa","global_name":null,"avatar":"a705470e31b1f95c7c8acda62c99249b","discriminator":"4261","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:17:44.572000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116430882086060082","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I get started?","last_message_id":"1116431249205112842","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:17:44.728000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:17:44.728000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","1072507804511899678"]}},{"id":"1116428525763838003","type":0,"content":"<@1072591948499664996> when using a custom agent through an agent executor, is it possible to pass in a argument to the callback function so that the argument value can be used when with  \"on_llm_start\"?","channel_id":"1072944049788555314","author":{"id":"757924024520015882","username":"Demba","global_name":null,"avatar":null,"discriminator":"0871","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:08:22.781000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116428525763838003","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"when using a custom agent through an agent executor, is it possible to pass in a argument","last_message_id":"1116430262423781426","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:08:23.054000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:08:23.054000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":5,"member_ids_preview":["1072591948499664996","757924024520015882","437808476106784770"]}},{"id":"1116427496745881640","type":0,"content":"<@1072591948499664996> how do i prevent the size of a table from breaking the token length?","channel_id":"1072944049788555314","author":{"id":"829503224661803018","username":"jonathandgough","global_name":null,"avatar":null,"discriminator":"2951","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:04:17.444000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116427496745881640","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i prevent the size of a table from breaking the token length?","last_message_id":"1116427647224905738","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:04:17.569000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:04:17.569000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["829503224661803018","1072591948499664996","437808476106784770"]}},{"id":"1116427310250340443","type":0,"content":"<@1072591948499664996> how do I change AI in ConversationBufferWindowMemory to another name","channel_id":"1072944049788555314","author":{"id":"400746939232026625","username":"Fj00 üá¶üá∂","global_name":null,"avatar":"5d67e2793b39c36e43b418c05fff5e2d","discriminator":"2139","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T18:03:32.980000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116427310250340443","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do I change AI in ConversationBufferWindowMemory to another name","last_message_id":"1116427487019286618","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T18:03:33.199000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T18:03:33.199000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","400746939232026625"]}},{"id":"1116423493228179586","type":0,"content":"<@1072591948499664996> How do you use BufferMemory with the ConversationalRetrievalQAChain in javascript?","channel_id":"1072944049788555314","author":{"id":"968981009016160346","username":"FEAT.R","global_name":null,"avatar":"5b0cc5adcb587754b8d352a5e6efbb74","discriminator":"6930","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:48:22.931000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116423493228179586","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do you use BufferMemory with the ConversationalRetrievalQAChain in javascript?","last_message_id":"1116428557707649144","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:48:23.128000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:48:23.128000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["968981009016160346","437808476106784770","1072591948499664996"]}},{"id":"1116421360542023791","type":0,"content":"<@1072591948499664996> How do you use memory with ChatOpenAI in python?","channel_id":"1072944049788555314","author":{"id":"251035647689621504","username":"Guru","global_name":null,"avatar":"55735a35c21085a9e4192a40e492b0d1","discriminator":"7018","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:39:54.459000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116421360542023791","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do you use memory with ChatOpenAI in python?","last_message_id":"1116421622925103296","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:39:54.641000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:39:54.641000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","251035647689621504"]}},{"id":"1116421162440867980","type":0,"content":"<@1072591948499664996> in javascript, how to get longer responses from VectorDBQA chain, and modify prompt template?\n\n    const chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n      k: 1,\n      returnSourceDocuments: true,\n    });\n    const response = await chain.call({ query: input });","channel_id":"1072944049788555314","author":{"id":"355553799181565953","username":"shawnsqvl","global_name":null,"avatar":"f3b785217113b959cc943008e776c394","discriminator":"7830","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:39:07.228000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116421162440867980","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"in javascript, how to get longer responses from VectorDBQA chain, and modify prompt templa","last_message_id":"1116421871462781040","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:39:07.424000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:39:07.424000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","437808476106784770","355553799181565953"]}},{"id":"1116420537372115036","type":0,"content":"<@1072591948499664996> How do I deal with a gpt4 chatbot that is hitting token limits due to large document sizes?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:36:38.200000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116420537372115036","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I deal with a gpt4 chatbot that is hitting token limits due to large document sizes","last_message_id":"1116420718679302234","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:36:38.745000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:36:38.745000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","161206345620652032","1072591948499664996"]}},{"id":"1116416589441532074","type":0,"content":"<@1072591948499664996>     qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=chain_type, retriever=retriever, return_source_documents=True)\nwhich model this is using?","channel_id":"1072944049788555314","author":{"id":"1073903225868124201","username":"Prasan","global_name":null,"avatar":null,"discriminator":"3660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:20:56.940000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116416589441532074","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=chain_type, retriever=retriever,","last_message_id":"1116416745259942038","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:20:57.078000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:20:57.078000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1073903225868124201","437808476106784770","1072591948499664996"]}},{"id":"1116414299586445412","type":0,"content":"<@1072591948499664996> what is the difference between chat_prompt and message_prompt","channel_id":"1072944049788555314","author":{"id":"1101776408507863150","username":"innotone","global_name":null,"avatar":null,"discriminator":"8935","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T17:11:50.996000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116414299586445412","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is the difference between chat_prompt and message_prompt","last_message_id":"1116414565094260848","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T17:11:51.121000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T17:11:51.121000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","1101776408507863150","437808476106784770"]}},{"id":"1116410978922409994","type":0,"content":"<@1072591948499664996> How to i create a bot tha read pdfs and then answers questions?","channel_id":"1072944049788555314","author":{"id":"465814189747732490","username":"Media","global_name":null,"avatar":"556d87b1871b7d86b5fc8bf4352ac5ca","discriminator":"1029","public_flags":64,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:58:39.288000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116410978922409994","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to i create a bot tha read pdfs and then answers questions?","last_message_id":"1116411191829479474","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:58:39.433000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:58:39.433000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","465814189747732490","437808476106784770"]}},{"id":"1116407287192490144","type":0,"content":"<@1072591948499664996> I use this:\nconst gMemory = new BufferMemory({\n    memoryKey: 'chat_history',\n    inputKey: 'question',\n    outputKey: 'text',\n    returnMessages: true,\n})\nconst vectorStore = await dbService.getVectorStore(memoryOption)\n        const chainOptions = {\n            questionGeneratorChainOptions: {\n                llm: model,\n            },\n            returnSourceDocuments: true,\n            memory: memoryOption ? gMemory : undefined,\n        }\nconst model = new OpenAI()\nconst gChain = ConversationalRetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), chainOptions)\nconst response = await gChain.call({ question: prompt })\nI get this error:\nError: Request failed with status code 400\n    at createError (C:\\dev_ai\\Langchain\\NextLangchain-main\\node_modules\\openai\\node_modules\\axios\\lib\\core\\createError.js:16:15)\n    at settle (C:\\dev_ai\\Langchain\\NextLangchain-main\\node_modules\\openai\\node_modules\\axios\\lib\\core\\settle.js:17:12)\n    at IncomingMessage.handleStreamEnd (C:\\dev_ai\\Langchain\\NextLangchain-main\\node_modules\\openai\\node_modules\\axios\\lib\\adapters\\http.js:322:11)\n    at IncomingMessage.emit (node:events:524:35)\n    at endReadableNT (node:internal/streams/readable:1359:12)\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {\n  config: {\n    transitional: {\n      silentJSONParsing: true,\n      forcedJSONParsing: true,\n      clarifyTimeoutError: false\n    },","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:43:59.111000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116407287192490144","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I use this:const gMemory = new BufferMemory({    memoryKey: 'chat_history',    inputKey","last_message_id":"1116413617231896667","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:43:59.437000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:43:59.437000+00:00"},"message_count":15,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":15,"member_ids_preview":["706926250723115019","1072591948499664996"]}},{"id":"1116407182326517941","type":0,"content":"<@&1072943855747481672>","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:43:34.109000+00:00","edited_timestamp":"2023-06-08T16:43:55.095000+00:00","flags":0,"components":[]},{"id":"1116406679286845523","type":0,"content":"<@1072591948499664996> how can i use gpt-4 model in a conversationchain with memorybuffer?","channel_id":"1072944049788555314","author":{"id":"330360172448841728","username":"Yago","global_name":null,"avatar":"153ee525053c5743e70bbbc9d97e6030","discriminator":"4202","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:41:34.175000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116406679286845523","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i use gpt-4 model in a conversationchain with memorybuffer?","last_message_id":"1116407784741802145","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:41:34.320000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:41:34.320000+00:00"},"message_count":8,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","330360172448841728"]}},{"id":"1116406604758265876","type":0,"content":"<@1072591948499664996> Is there a prompt template builder in langchain which takes \"prefix\" and \"suffix\" as input while building prompt","channel_id":"1072944049788555314","author":{"id":"1101776408507863150","username":"innotone","global_name":null,"avatar":null,"discriminator":"8935","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:41:16.406000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116406604758265876","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is there a prompt template builder in langchain which takes \"prefix\" and \"suffix\" as input","last_message_id":"1116406835306582026","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:41:16.569000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:41:16.569000+00:00"},"message_count":3,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1101776408507863150","1072591948499664996"]}},{"id":"1116406569060544684","type":0,"content":"how can i use gpt-4 model in a conversationchain with memorybuffer?","channel_id":"1072944049788555314","author":{"id":"330360172448841728","username":"Yago","global_name":null,"avatar":"153ee525053c5743e70bbbc9d97e6030","discriminator":"4202","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:41:07.895000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116405791046504539","type":0,"content":"<@1072591948499664996> how to use input_documents with QA chains?","channel_id":"1072944049788555314","author":{"id":"378791112736899073","username":"0xnuts","global_name":"0xnuts","avatar":"d2050091b956250174e2f8f6633c4f3a","discriminator":"1004","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:38:02.402000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116405791046504539","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to use input_documents with QA chains?","last_message_id":"1116409259962400828","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:38:02.907000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:38:02.907000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","378791112736899073"]}},{"id":"1116398854200373510","type":0,"content":"<@1072591948499664996> If my dataframe df already has an embeddings column how would I change this to use those embeddings instead of generating new ones? \"from langchain.schema import Document\n\ndocuments = []\nfor index, row in df.iterrows():\n    body = row['combined']\n    metadata = row.to_dict()\n    del metadata['combined']\n    del metadata['embedding']\n    documents.append(Document(page_content=body, metadata=metadata))\nembeddings = OpenAIEmbeddings()\nchroma_db = Chroma.from_documents(documents=documents, embedding=embeddings)\"","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:10:28.529000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116398854200373510","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"If my dataframe df already has an embeddings column how would I change this to use those e","last_message_id":"1116398931564310548","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:10:28.806000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:10:28.806000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["161206345620652032","437808476106784770","1072591948499664996"]}},{"id":"1116397841208844379","type":0,"content":"<@1072591948499664996> how do i make this work with the current input and output keys from savecontext in base memory:(chain is conversationalretrievalqachain)\nasync function query(prompt) {\n    try {\n        gChatHistory.push({ user: prompt })\n        const response = await gChain.call({ [gCallType]: prompt, chat_history: gChatHistory })\n        gChatHistory.push({ gpt: response })\n        sse.send(null, 'end')\n        return response\n    } catch (error) {\n        console.error('An error occurred while querying:', error)\n        throw error\n    }\n}","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T16:06:27.013000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116397841208844379","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i make this work with the current input and output keys from savecontext in base me","last_message_id":"1116399667857592552","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T16:06:27.149000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T16:06:27.149000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","1072591948499664996","706926250723115019"]}},{"id":"1116393741733863434","type":0,"content":"<@1072591948499664996> How can I use an Output Parser for my LLMChain tool? My LLMChain tool contains an LLM to generate a list of information based on user input. I'm using my LLMChain tool in a CHAT_CONVERSATIONAL_REACT_DESCRIPTION agent.","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T15:50:09.622000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116393741733863434","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I use an Output Parser for my LLMChain tool? My LLMChain tool contains an LLM to g","last_message_id":"1116394940721811627","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T15:50:09.770000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T15:50:09.770000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["454324403161923604","437808476106784770","1072591948499664996"]}},{"id":"1116388221132222464","type":0,"content":"<@1072591948499664996> how to route between agents with their tools?","channel_id":"1072944049788555314","author":{"id":"242709790969692174","username":"raphaelwcosta","global_name":"Raphael Costa","avatar":"302f58ac6d89ae73a8cbafd26897d66f","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T15:28:13.408000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116388221132222464","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to route between agents with their tools?","last_message_id":"1116388451365949450","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T15:28:13.622000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T15:28:13.622000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["242709790969692174","437808476106784770","1072591948499664996"]}},{"id":"1116386151318696056","type":0,"content":"<@1072591948499664996> I've defined a custom tool using the @tool decorator. How can I modify the `on_tool_end` callback so that I can do something custom after the tool is done running?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T15:19:59.926000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116386151318696056","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I've defined a custom tool using the @tool decorator. How can I modify the `on_tool_end` c","last_message_id":"1116386233426391140","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T15:20:00.053000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T15:20:00.053000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["359130672872423464","437808476106784770","1072591948499664996"]}},{"id":"1116385916018249900","type":0,"content":"<@1072591948499664996> conver this to a conversationalretrevalqachain that has memory:\nasync function getVectorChain(model, memoryOption = false) {\n    try {\n        const { vectorStore, vectorMemory } = await dbService.getVectorStore(memoryOption)\n        gMemory = vectorMemory\n        const vectorOptions = {\n            k: 1,\n            returnSourceDocuments: true,\n            memory: memoryOption ? gMemory : undefined,\n        }\n        return VectorDBQAChain.fromLLM(model, vectorStore, vectorOptions)\n    } catch (error) {\n        console.error('An error occurred while getting vector chain:', error)\n        throw error\n    }\n}","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T15:19:03.826000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116385916018249900","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"conver this to a conversationalretrevalqachain that has memory:async function getVectorCh","last_message_id":"1116386171623313479","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T15:19:03.992000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T15:19:03.992000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["706926250723115019","1072591948499664996","437808476106784770"]}},{"id":"1116380060312666203","type":0,"content":"<@1072591948499664996> How can I specify what should be outputted in an LLMChain tool? Python only.","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T14:55:47.717000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116380060312666203","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I specify what should be outputted in an LLMChain tool? Python only.","last_message_id":"1116382902452752414","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T14:55:47.923000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T14:55:47.923000+00:00"},"message_count":16,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":16,"member_ids_preview":["454324403161923604","437808476106784770","1072591948499664996"]}},{"id":"1116378674992795779","type":0,"content":"<@1072591948499664996> I'm trying to run LangChain with the existing Weaviate instance. I have a class named `ClientMacro`. I try to do a basic documents query with `similaritySearch` as stated here: https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/weaviate#usage-query-documents. The relevant chunk of code is as follows:\n\n```\n  const vectorStore = await WeaviateStore.fromExistingIndex(\n    new OpenAIEmbeddings(),\n    {\n      client: weaviateClient,\n      indexName: 'ClientMacro',\n    },\n  );\n  const results = await vectorStore.similaritySearch('hello world', 1);\n```\n\nBut I get this error: `Error in similaritySearch' Error: explorer: get class: vector search: object vector search at index clientmacro: shard clientmacro_b6vs7sSm0XLH: vector search: knn search: distance between entrypoint and query node: vector lengths don't match: 300 vs 1536`.\n\nCan you guide me on what I am doing wrong?","channel_id":"1072944049788555314","author":{"id":"128604800156696576","username":"evenfrost","global_name":"Evenfrost","avatar":"5295f4fbc2c840de9438c21db2407573","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[{"type":"article","url":"https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/weaviate","title":"Weaviate | ü¶úÔ∏èüîó Langchain","description":"Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.","thumbnail":{"url":"https://js.langchain.com/img/parrot-chainlink-icon.png","proxy_url":"https://images-ext-1.discordapp.net/external/m6Ms7Dj-wXClNMfVtVyxPkjpx6rzqkjG_wx6e2MNRF0/https/js.langchain.com/img/parrot-chainlink-icon.png","width":794,"height":436}}],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T14:50:17.431000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116378674992795779","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I'm trying to run LangChain with the existing Weaviate instance. I have a class named `Cli","last_message_id":"1116386686935519262","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T14:50:17.709000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T14:50:17.709000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","128604800156696576","1072591948499664996"]}},{"id":"1116372364616269864","type":0,"content":"<@1072591948499664996> why is this function never finishing: async function uploadTexts(texts) {\n    await PineconeStore.fromTexts(texts, [], new OpenAIEmbeddings(), {\n        pineconeIndex: gClientIndex,\n    })\n    return true\n}","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T14:25:12.920000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116372364616269864","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"why is this function never finishing: async function uploadTexts(texts) {    await Pineco","last_message_id":"1116372559852748951","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T14:25:13.175000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T14:25:13.175000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","706926250723115019"]}},{"id":"1116372206637822112","type":0,"content":"<@1072591948499664996>  When performing a similarity search in pinecone with Langchain, I get documents returned that are not similar to the base document being used for comparison. How can I identify the similar documents and those that are not similar based on all the documents returned. I am using this code to compare documents one by one.\n\npinecone_vectorstore = search\n\nfor i, base_document in enumerate(documents):\n    base_document_content = base_document.page_content\n    similar_docs = pinecone_vectorstore.similarity_search(query=base_document_content, k=len(documents))\n    print(similar_docs)","channel_id":"1072944049788555314","author":{"id":"997795325261664266","username":"nyx-12","global_name":null,"avatar":null,"discriminator":"1868","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T14:24:35.255000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116372206637822112","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"When performing a similarity search in pinecone with Langchain, I get documents returned t","last_message_id":"1116374355362979920","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T14:24:35.397000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T14:24:35.397000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["997795325261664266","437808476106784770","1072591948499664996"]}},{"id":"1116367293316866088","type":0,"content":"<@1072591948499664996> how to change this to text instead of docs:\nawait PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n        pineconeIndex: gClientIndex,\n    })","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T14:05:03.828000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116367293316866088","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to change this to text instead of docs:await PineconeStore.fromDocuments(docs, new Op","last_message_id":"1116367441228992562","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T14:05:04.056000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T14:05:04.056000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","706926250723115019","437808476106784770"]}},{"id":"1116363916147171398","type":0,"content":"<@1072591948499664996> Is it possible to write a book using langchain?","channel_id":"1072944049788555314","author":{"id":"769809289187295254","username":"Seja-sama","global_name":null,"avatar":"275fa9a318e44561275fe8dcbfaf21ca","discriminator":"1482","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T13:51:38.648000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116363916147171398","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is it possible to write a book using langchain?","last_message_id":"1116364108510527498","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T13:51:38.941000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T13:51:38.941000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","769809289187295254"]}},{"id":"1116360673325224047","type":0,"content":"<@1072591948499664996> I have several documents stored in a pinecone database, I want to compare if the documents are similar and if there are different documents in terms of content in the documents. How can I compare the documents one by one to identify if they talk about the same thing using Langchain. Also How can I perform sentiment analysis after identifying the similar documents? Please give an example in python.","channel_id":"1072944049788555314","author":{"id":"997795325261664266","username":"nyx-12","global_name":null,"avatar":null,"discriminator":"1868","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T13:38:45.499000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116360673325224047","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have several documents stored in a pinecone database, I want to compare if the documents","last_message_id":"1116371436613943407","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T13:38:45.625000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T13:38:45.625000+00:00"},"message_count":23,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":23,"member_ids_preview":["437808476106784770","997795325261664266","1072591948499664996"]}},{"id":"1116355987981475841","type":0,"content":"<@1072591948499664996> what's the difference between:\nConversationalRetrievalQAChain.fromLLM\nand\nVectorDBQAChain.fromLLM","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T13:20:08.426000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116355987981475841","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what's the difference between:ConversationalRetrievalQAChain.fromLLMandVectorDBQAChain.","last_message_id":"1116379055156113459","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T13:20:08.620000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T13:20:08.620000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","437808476106784770","706926250723115019"]}},{"id":"1116351795426500729","type":0,"content":"<@1072591948499664996> \nI want to add custom prompts to my below code:\n`    search = SerpAPIWrapper(params=serp_params)\n    llm = OpenAI(model_name=selected_model, temperature=0)\n\n    search_tool = Tool(\n        name=\"web_search\",\n        description=\"A search engine. Useful for when you need to answer government related questions. Input should be a search query.\",\n        func=search.run,\n        return_direct=True\n    )\n\n    tools = [search_tool]\n\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", ai_prefix='Simba')\n    agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=False,\n                                   memory=memory, max_iterations=5, early_stopping_method=\"generate\")`","channel_id":"1072944049788555314","author":{"id":"1077102711025184819","username":"muhammad_daniyal","global_name":null,"avatar":null,"discriminator":"6245","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T13:03:28.843000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116351795426500729","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I want to add custom prompts to my below code:`    search = SerpAPIWrapper(params=serp_pa","last_message_id":"1116356717215748126","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T13:03:29.038000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T13:03:29.038000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1077102711025184819","1072591948499664996","437808476106784770"]}},{"id":"1116345689417449472","type":0,"content":"<@1072591948499664996> how can i change the AgentOutputParser so i won't have this error:\nOutputParserException: Could not parse LLM output ...\ni am using create_csv_agent to create the agent.","channel_id":"1072944049788555314","author":{"id":"1061886480974352394","username":"yoyotheone","global_name":null,"avatar":null,"discriminator":"4786","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T12:39:13.057000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116345689417449472","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i change the AgentOutputParser so i won't have this error:OutputParserException:","last_message_id":"1116345995031220316","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T12:39:13.162000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T12:39:13.162000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","1061886480974352394","437808476106784770"]}},{"id":"1116335363267514508","type":0,"content":"<@1072591948499664996>  , what is agents and what are the uses of it","channel_id":"1072944049788555314","author":{"id":"1105098001845862410","username":"satish","global_name":null,"avatar":null,"discriminator":"5163","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:58:11.111000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116335363267514508","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":", what is agents and what are the uses of it","last_message_id":"1116335561934909480","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:58:11.302000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:58:11.302000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1105098001845862410","1072591948499664996"]}},{"id":"1116334802430337034","type":0,"content":"<@1072591948499664996>  how to do question answer from csv file?","channel_id":"1072944049788555314","author":{"id":"1077102711025184819","username":"muhammad_daniyal","global_name":null,"avatar":null,"discriminator":"6245","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:55:57.397000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116334802430337034","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to do question answer from csv file?","last_message_id":"1116337723754692768","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:55:57.648000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:55:57.648000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["1072591948499664996","1077102711025184819","437808476106784770"]}},{"id":"1116334385323577485","type":0,"content":"<@1072591948499664996> , how to pass dynamic values to the answers given by my chatbot","channel_id":"1072944049788555314","author":{"id":"1105098001845862410","username":"satish","global_name":null,"avatar":null,"discriminator":"5163","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:54:17.951000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116334385323577485","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":", how to pass dynamic values to the answers given by my chatbot","last_message_id":"1116335066877022269","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:54:18.142000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:54:18.142000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","1072591948499664996","1105098001845862410"]}},{"id":"1116334052006436885","type":0,"content":"<@1072591948499664996>  i have built a chatbot using vector embeddings,load_qa_chain,openai llm and conersationbufferwindowmemory .\n\n\nI need to pass dynamic values to the response once response is finalized by open ai , how can i pass the dynamic values ?","channel_id":"1072944049788555314","author":{"id":"1105098001845862410","username":"satish","global_name":null,"avatar":null,"discriminator":"5163","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:52:58.482000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116334052006436885","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i have built a chatbot using vector embeddings,load_qa_chain,openai llm and conersationbuf","last_message_id":"1116334127923335198","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:52:58.634000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:52:58.634000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","1105098001845862410"]}},{"id":"1116333139665625139","type":0,"content":"<@1072591948499664996>  , i have built a chatbot using vector embeddings,load_qa_chain,openai llm and conersationbufferwindowmemory .\n\n\nI need to pass dynamic values to the response once response is finalized by open ai , how can i pass the dynamic values ?","channel_id":"1072944049788555314","author":{"id":"1105098001845862410","username":"satish","global_name":null,"avatar":null,"discriminator":"5163","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:49:20.963000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116333139665625139","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":", i have built a chatbot using vector embeddings,load_qa_chain,openai llm and conersationb","last_message_id":"1116333227288825886","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:49:21.099000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:49:21.099000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1105098001845862410","437808476106784770","1072591948499664996"]}},{"id":"1116331327441092619","type":0,"content":"<@1072591948499664996> how can I load a langchain document into llhamaindex using python ?","channel_id":"1072944049788555314","author":{"id":"882357742256406608","username":"Ketan","global_name":null,"avatar":"02d98104c2603e03428e55fc2c1a428b","discriminator":"5442","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:42:08.895000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116331327441092619","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I load a langchain document into llhamaindex using python ?","last_message_id":"1116331539110821929","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:42:09.070000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:42:09.070000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","882357742256406608"]}},{"id":"1116322148022239332","type":0,"content":"<@1072591948499664996> How to use a custom tool with ConversationalRetrievalChain?","channel_id":"1072944049788555314","author":{"id":"1045597867714293790","username":"jerpic","global_name":null,"avatar":null,"discriminator":"8123","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T11:05:40.351000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116322148022239332","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to use a custom tool with ConversationalRetrievalChain?","last_message_id":"1116328713894764634","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T11:05:40.557000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T11:05:40.557000+00:00"},"message_count":10,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["1072591948499664996","437808476106784770","1045597867714293790"]}},{"id":"1116320507009192077","type":0,"content":"<@1072591948499664996> how can I load a langchain document into llhamaindex ?","channel_id":"1072944049788555314","author":{"id":"882357742256406608","username":"Ketan","global_name":null,"avatar":"02d98104c2603e03428e55fc2c1a428b","discriminator":"5442","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T10:59:09.103000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116320507009192077","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I load a langchain document into llhamaindex ?","last_message_id":"1116321290513547294","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T10:59:09.272000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T10:59:09.272000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["437808476106784770","1072591948499664996","882357742256406608"]}},{"id":"1116319032174444625","type":0,"content":"<@1072591948499664996> I am struggling to get my implementation to follow the instrutions in a router chain. What hints do you have for getting the LLM to not get confused as to which destination_chain to select?","channel_id":"1072944049788555314","author":{"id":"710219082095525949","username":"AB","global_name":null,"avatar":null,"discriminator":"1571","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T10:53:17.475000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116319032174444625","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am struggling to get my implementation to follow the instrutions in a router chain. What","last_message_id":"1116320097796100096","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T10:53:17.724000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T10:53:17.724000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","710219082095525949"]}},{"id":"1116313105660653688","type":0,"content":"<@1072591948499664996> do you need to explicitly pass relevant documents to chain like ConversationalRetrievalChain or it automatically take cares of that when we pass the db's retriever object","channel_id":"1072944049788555314","author":{"id":"896472922363928586","username":"shaktiman","global_name":null,"avatar":"8c20c933ab5c5709180ba44104bb0f7c","discriminator":"2731","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T10:29:44.484000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116313105660653688","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"do you need to explicitly pass relevant documents to chain like ConversationalRetrievalCha","last_message_id":"1116588628978696202","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T10:29:44.651000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T10:29:44.651000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":5,"member_ids_preview":["896472922363928586","437808476106784770","1072591948499664996"]}},{"id":"1116306413451497562","type":0,"content":"<@1072591948499664996> I want to import a custom function to the PythonAstReplTool, how can i do it","channel_id":"1072944049788555314","author":{"id":"803143534022098954","username":"chewan21","global_name":null,"avatar":null,"discriminator":"3505","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T10:03:08.937000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116306413451497562","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I want to import a custom function to the PythonAstReplTool, how can i do it","last_message_id":"1116306504505622538","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T10:03:09.119000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T10:03:09.119000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","803143534022098954","437808476106784770"]}},{"id":"1116306361676988456","type":0,"content":"I want to import a custom function to the PythonAstReplTool, how can i do it","channel_id":"1072944049788555314","author":{"id":"803143534022098954","username":"chewan21","global_name":null,"avatar":null,"discriminator":"3505","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T10:02:56.593000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116295849157345280","type":0,"content":"<@1072591948499664996> please give some examples for JSONLoader in python","channel_id":"1072944049788555314","author":{"id":"1073903225868124201","username":"Prasan","global_name":null,"avatar":null,"discriminator":"3660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T09:21:10.213000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116295849157345280","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"please give some examples for JSONLoader in python","last_message_id":"1116299641198215248","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T09:21:10.334000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T09:21:10.334000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","1073903225868124201","437808476106784770"]}},{"id":"1116294236506177556","type":0,"content":"<@1072591948499664996> how can I generate all the questions that can be asked from a text body or  a  text document ?","channel_id":"1072944049788555314","author":{"id":"882357742256406608","username":"Ketan","global_name":null,"avatar":"02d98104c2603e03428e55fc2c1a428b","discriminator":"5442","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T09:14:45.727000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116294236506177556","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I generate all the questions that can be asked from a text body or  a  text docume","last_message_id":"1116294766158684223","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T09:14:45.864000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T09:14:45.864000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["882357742256406608","437808476106784770","1072591948499664996"]}},{"id":"1116289156818145350","type":0,"content":"<@1072591948499664996> turn verbose on in LLM chain","channel_id":"1072944049788555314","author":{"id":"818461844741029940","username":"segneck","global_name":"Segneck","avatar":"a_a8142e1951c5170437e18561c2cd21ba","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T08:54:34.635000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116289156818145350","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"turn verbose on in LLM chain","last_message_id":"1116289621152776303","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T08:54:34.788000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T08:54:34.788000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["818461844741029940","1072591948499664996","437808476106784770"]}},{"id":"1116276213481222144","type":0,"content":"<@1072591948499664996> what is {agent_scratchpad} used for?","channel_id":"1072944049788555314","author":{"id":"915229846592946237","username":"Niksta -------->>> Let‚Äôs go! üî•","global_name":null,"avatar":"421a127232684dfcfe183a2152384481","discriminator":"5365","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T08:03:08.703000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116276213481222144","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what is {agent_scratchpad} used for?","last_message_id":"1116279025535561788","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T08:03:08.884000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T08:03:08.884000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["915229846592946237","437808476106784770","1072591948499664996"]}},{"id":"1116270182768316487","type":0,"content":"<@1072591948499664996>  how can i create a custom chain that takes in a summarised chat history and also embeddings from an index?","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T07:39:10.869000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116270182768316487","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i create a custom chain that takes in a summarised chat history and also embedding","last_message_id":"1116270470883446796","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T07:39:11.092000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T07:39:11.092000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["839215717693259816","1072591948499664996","437808476106784770"]}},{"id":"1116269171324497961","type":0,"content":"<@1072591948499664996> How can I create text generation with prompts and VectorStore?","channel_id":"1072944049788555314","author":{"id":"701860446755487744","username":"AlexChaptykov","global_name":null,"avatar":null,"discriminator":"4941","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T07:35:09.722000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116269171324497961","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I create text generation with prompts and VectorStore?","last_message_id":"1116277558602891304","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T07:35:09.926000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T07:35:09.926000+00:00"},"message_count":5,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":5,"member_ids_preview":["1072591948499664996","437808476106784770","701860446755487744"]}},{"id":"1116269073404276846","type":0,"content":"How can I create text generation with prompts and VectorStore?","channel_id":"1072944049788555314","author":{"id":"701860446755487744","username":"AlexChaptykov","global_name":null,"avatar":null,"discriminator":"4941","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T07:34:46.376000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116265659031105616","type":0,"content":"<@1072591948499664996> how do i access action and action input inside a llm?","channel_id":"1072944049788555314","author":{"id":"502145425206411332","username":"sudde","global_name":null,"avatar":"5f709633ed0c1045ba3af4278db2dd0c","discriminator":"0040","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T07:21:12.326000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116265659031105616","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i access action and action input inside a llm?","last_message_id":"1116277776471822387","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T07:21:12.851000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T07:21:12.851000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["502145425206411332","1072591948499664996","437808476106784770"]}},{"id":"1116259062972157952","type":0,"content":"<@1072591948499664996> how do i view the entire prompt i am passing to the LLM before it gets sent?","channel_id":"1072944049788555314","author":{"id":"955842062639583313","username":"Rusty Shackleford","global_name":null,"avatar":"db2851fe179715b2bfd6ab045dd6ec48","discriminator":"4883","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T06:54:59.703000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116259062972157952","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i view the entire prompt i am passing to the LLM before it gets sent?","last_message_id":"1116259307470733342","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T06:55:00.102000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T06:55:00.102000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["955842062639583313","1072591948499664996","437808476106784770"]}},{"id":"1116258108822532126","type":0,"content":"<@1072591948499664996> i have my context ready can i use chain_type = map_reduce without the retriever?","channel_id":"1072944049788555314","author":{"id":"458856341897740310","username":"yashs","global_name":null,"avatar":null,"discriminator":"8846","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T06:51:12.216000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116258108822532126","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i have my context ready can i use chain_type = map_reduce without the retriever?","last_message_id":"1116258184978501663","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T06:51:12.904000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T06:51:12.904000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","458856341897740310"]}},{"id":"1116237440223547472","type":0,"content":"<@1072591948499664996> can i pass in my own prompt into a conversationalretrievalchain? python","channel_id":"1072944049788555314","author":{"id":"955842062639583313","username":"Rusty Shackleford","global_name":null,"avatar":"db2851fe179715b2bfd6ab045dd6ec48","discriminator":"4883","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T05:29:04.438000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116237440223547472","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"can i pass in my own prompt into a conversationalretrievalchain? python","last_message_id":"1116237597191176263","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T05:29:04.608000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T05:29:04.608000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","955842062639583313","437808476106784770"]}},{"id":"1116223345399185418","type":0,"content":"<@1072591948499664996> initialize_agent„Å´„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÇíËøΩÂä†„Åô„ÇãÊñπÊ≥ï„ÅØÔºü","channel_id":"1072944049788555314","author":{"id":"1092624100998004838","username":"RY","global_name":null,"avatar":"4c1cf5f618b2b760ebeb6977c17183e7","discriminator":"4944","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T04:33:03.970000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116223345399185418","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"initialize_agent„Å´„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÇíËøΩÂä†„Åô„ÇãÊñπÊ≥ï„ÅØÔºü","last_message_id":"1116306766582530058","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T04:33:04.164000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T04:33:04.164000+00:00"},"message_count":22,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":22,"member_ids_preview":["1092624100998004838","1072591948499664996","437808476106784770"]}},{"id":"1116209892517232640","type":0,"content":"<@1072591948499664996> help me add a coustom prompt to agent = initialize_agent(\n    llm=llm,\n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    tools=tools,\n    verbose=True,\n    max_iterations=5,\n    early_stopping_methood='genarate',\n    memory=conversation_memory,\n\n    )","channel_id":"1072944049788555314","author":{"id":"861363872812105772","username":"debug.log","global_name":null,"avatar":"3054e7a943f5de922b8d23a81ad243eb","discriminator":"3406","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T03:39:36.553000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116209892517232640","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"help me add a coustom prompt to agent = initialize_agent(    llm=llm,    agent=AgentType","last_message_id":"1116211262137831454","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T03:39:36.677000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T03:39:36.677000+00:00"},"message_count":7,"member_count":2,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","861363872812105772"]}},{"id":"1116204243750240277","type":0,"content":"<@1072591948499664996> I'm using a LangChain agent that interacts with a vectorstore using a tool. When it is used during an AgentExecutor chain, it returns a tuple of the string response given by the tool according to the input query (also a string) and a list of sources in shorthand. I want to be able to access that source information AFTER the tool's chain finishes but BEFORE the main agent receives that response as an 'Observation'. How can I do that?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T03:17:09.782000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116204243750240277","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I'm using a LangChain agent that interacts with a vectorstore using a tool. When it is use","last_message_id":"1116204314071945267","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T03:17:09.988000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T03:17:09.988000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","359130672872423464","437808476106784770"]}},{"id":"1116202506662457374","type":0,"content":"<@1072591948499664996>  please give me a javascript example of chunking a large amount of data and feeding to openai embeddings to store into pinecone db","channel_id":"1072944049788555314","author":{"id":"875067761557127178","username":"gautam","global_name":null,"avatar":"008933f884b722ddd0427b7c18e51940","discriminator":"1081","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T03:10:15.628000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116202506662457374","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"please give me a javascript example of chunking a large amount of data and feeding to open","last_message_id":"1116202824909463552","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T03:10:15.785000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T03:10:15.785000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","437808476106784770","875067761557127178"]}},{"id":"1116201485722722394","type":0,"content":"<@1072591948499664996> i am not able to split json text into chunks","channel_id":"1072944049788555314","author":{"id":"1073903225868124201","username":"Prasan","global_name":null,"avatar":null,"discriminator":"3660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T03:06:12.217000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116201485722722394","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i am not able to split json text into chunks","last_message_id":"1116202340110827593","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T03:06:12.804000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T03:06:12.804000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","1073903225868124201","437808476106784770"]}},{"id":"1116200315365101648","type":0,"content":"<@1072591948499664996> why is langchain sending a large openai embeddings call with over 3m characters, but the chunks are all less than 2000 characters. \n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 2000,\n  chunkOverlap: 100,\n});\n\nlet docs = [];\nfor (const d of data) {\n  const docOutput = textSplitter.splitText(d);\n  if(docOutput.length > 1) {\n  docs = [...docs, ...docOutput];\n  } else {\n  docs.push(d);\n  }\n}\n\n\nconsole.log(\"Initializing Store...\");\n\nlet store;\ntry {\nif(pineconeIndex) {\n  await PineconeStore.fromTexts(\n    docs,\n    docs.map((doc,i) => {return {id: i+1}}),\n    new OpenAIEmbeddings({\n      openAIApiKey: process.env.OPENAI_API_KEY\n    }),\n    {\n      pineconeIndex,\n    }\n  )\n}\n\nhere data is an array of large pdf contents (200+ pages)","channel_id":"1072944049788555314","author":{"id":"875067761557127178","username":"gautam","global_name":null,"avatar":"008933f884b722ddd0427b7c18e51940","discriminator":"1081","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T03:01:33.182000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116200315365101648","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"why is langchain sending a large openai embeddings call with over 3m characters, but the c","last_message_id":"1116200450992119910","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T03:01:33.302000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T03:01:33.302000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["875067761557127178","437808476106784770","1072591948499664996"]}},{"id":"1116199567650729984","type":0,"content":"<@1072591948499664996> how can I intercept the observation made by a tool BEFORE it reaches the main agent chain?","channel_id":"1072944049788555314","author":{"id":"359130672872423464","username":"gnp","global_name":null,"avatar":null,"discriminator":"7148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T02:58:34.913000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116199567650729984","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I intercept the observation made by a tool BEFORE it reaches the main agent chain?","last_message_id":"1116199725239111700","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T02:58:35.109000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T02:58:35.109000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","359130672872423464"]}},{"id":"1116192266227286086","type":0,"content":"<@1072591948499664996> how can I call this ConversationalRetrievalQAChain?\n```\nconst model = new OpenAI({ openAIApiKey: OPENAI_API_KEY, temperature: 0.3 });\n  const promptA = new PromptTemplate({\n    template: 'Quien es {name}?',\n    inputVariables: ['name'],\n  });\n\n  // docs: https://js.langchain.com/docs/modules/chains/index_related_chains/conversational_retrieval\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever(),\n    {\n      qaChainOptions: {\n        type: 'stuff',\n        prompt: promptA,\n      },\n      memory: new BufferMemory({\n        memoryKey: 'chat_history', // Must be set to \"chat_history\"\n      }),\n    }\n  );\n  return chain;\n```","channel_id":"1072944049788555314","author":{"id":"697612541123362826","username":"KaiTakami","global_name":null,"avatar":"c795b2536c4c3e6b4b9354955acadcfd","discriminator":"9891","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T02:29:34.118000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116192266227286086","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I call this ConversationalRetrievalQAChain?```const model = new OpenAI({ openAIA","last_message_id":"1116193873857880145","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T02:29:34.286000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T02:29:34.286000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["1072591948499664996","697612541123362826","437808476106784770"]}},{"id":"1116189598654156800","type":0,"content":"<@1072591948499664996> How can I pass a custom prompt to a conversational retrieval qa chain in typescript and call the chain using my variables?","channel_id":"1072944049788555314","author":{"id":"697612541123362826","username":"KaiTakami","global_name":null,"avatar":"c795b2536c4c3e6b4b9354955acadcfd","discriminator":"9891","public_flags":128,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T02:18:58.119000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116189598654156800","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I pass a custom prompt to a conversational retrieval qa chain in typescript and ca","last_message_id":"1116191103096475679","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T02:18:58.420000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T02:18:58.420000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","697612541123362826","1072591948499664996"]}},{"id":"1116176410487312536","type":0,"content":"<@1072591948499664996> How do i pass a specific memory module to an agent?","channel_id":"1072944049788555314","author":{"id":"351421812791312385","username":"Ichig√∏","global_name":"Ichig√∏","avatar":"850c96e7782dc2f82851f7322b86c58a","discriminator":"3766","public_flags":512,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T01:26:33.815000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116176410487312536","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do i pass a specific memory module to an agent?","last_message_id":"1116218813566951575","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-08T01:26:34.007000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-08T01:26:34.007000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["437808476106784770","1072591948499664996","351421812791312385"]}},{"id":"1116159512865411212","type":0,"content":"@kappa.ai why does this code: ```const { ConversationChain } = require(\"langchain/chains\");\nconst {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} = require(\"langchain/prompts\");\nconst dotenv = require(\"dotenv\");\ndotenv.config();\n\n// takes in queryPinecone() return as arg qResMatches takes in openAIllm() return as arg _llmDefinition\nconst prompt = async (_llmDefinition, _question, contractData) => {\n  const messages = await ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n.....<my systemprompt>\n    Name:{contractName},Description: {description}, Source Code: {sourceCode} Audit: {audit}`\n    ),\n    // new MessagesPlaceholder(\"history\"), // Built in memory will inject history messages here\n    HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n  ]);\n\n  const chain = new ConversationChain({\n    prompt: messages,\n    llm: _llmDefinition,\n  });\n\n  console.log(`Asking question: ${_question}...`);\n\n  const queryRes = await chain.call({\n    input: _question,\n    contractName: contractData.name,\n    description: contractData.description,\n    audit: contractData.audit,\n    sourceCode: contractData.sourceCode,\n  });\n\n  return queryRes;\n};``` throw this error :Error: input values have 5 keys, you must specify an input key or pass only 1 key as input","channel_id":"1072944049788555314","author":{"id":"864892474891698177","username":"Ae_0h","global_name":null,"avatar":"ecf580a9e8de6e267f972b2360ce491a","discriminator":"0772","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-08T00:19:25.108000+00:00","edited_timestamp":"2023-06-08T00:19:44.972000+00:00","flags":0,"components":[]},{"id":"1116153619335622697","type":0,"content":"<@1072591948499664996> model() async request","channel_id":"1072944049788555314","author":{"id":"486716962177810481","username":"mac_man","global_name":null,"avatar":"f4fd74f7287c6fdf5079f98de06b84fc","discriminator":"7862","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T23:55:59.981000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116153619335622697","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"model() async request","last_message_id":"1116155352535277718","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T23:56:00.119000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T23:56:00.119000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["437808476106784770","1072591948499664996","486716962177810481"]}},{"id":"1116153480814530580","type":0,"content":"<@1072591948499664996> how to pass system message to gpt4 with langchain?","channel_id":"1072944049788555314","author":{"id":"1057969139932729344","username":"Fred B","global_name":null,"avatar":null,"discriminator":"1660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T23:55:26.955000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116153480814530580","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to pass system message to gpt4 with langchain?","last_message_id":"1116153645222871141","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T23:55:27.189000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T23:55:27.189000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","1057969139932729344","437808476106784770"]}},{"id":"1116151778883407934","type":0,"content":"<@1072591948499664996> how to use transformers embeddings filter, embeddings_function","channel_id":"1072944049788555314","author":{"id":"955842062639583313","username":"Rusty Shackleford","global_name":null,"avatar":"db2851fe179715b2bfd6ab045dd6ec48","discriminator":"4883","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T23:48:41.183000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116151778883407934","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to use transformers embeddings filter, embeddings_function","last_message_id":"1116230995776122880","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T23:48:41.386000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T23:48:41.386000+00:00"},"message_count":13,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":13,"member_ids_preview":["955842062639583313","1072591948499664996","437808476106784770"]}},{"id":"1116136507003113522","type":0,"content":"<@1072591948499664996> How can I see the prompt used by  llm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n)","channel_id":"1072944049788555314","author":{"id":"1100941971633213500","username":"Toots_McGhee","global_name":null,"avatar":null,"discriminator":"4933","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:48:00.083000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116136507003113522","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I see the prompt used by  llm = ChatOpenAI(    model_name=\"gpt-3.5-turbo\",    te","last_message_id":"1116136693108580363","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T22:48:00.231000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T22:48:00.231000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1072591948499664996","1100941971633213500"]}},{"id":"1116136419480567938","type":0,"content":"<@1072591948499664996> how to pass two input agrument to partial method of prompt template?","channel_id":"1072944049788555314","author":{"id":"1057969139932729344","username":"Fred B","global_name":null,"avatar":null,"discriminator":"1660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:47:39.216000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116136419480567938","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to pass two input agrument to partial method of prompt template?","last_message_id":"1116136590574637096","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T22:47:39.402000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T22:47:39.402000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1057969139932729344","437808476106784770","1072591948499664996"]}},{"id":"1116136375020953691","type":0,"content":"how to pass two input agrument to partial method of prompt template?","channel_id":"1072944049788555314","author":{"id":"1057969139932729344","username":"Fred B","global_name":null,"avatar":null,"discriminator":"1660","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:47:28.616000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116135106856374373","type":0,"content":"<@1072591948499664996> How can I change the chain_type to map_reduce in this code using model_kwargs? llm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n)","channel_id":"1072944049788555314","author":{"id":"1100941971633213500","username":"Toots_McGhee","global_name":null,"avatar":null,"discriminator":"4933","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:42:26.262000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116135106856374373","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I change the chain_type to map_reduce in this code using model_kwargs? llm = ChatO","last_message_id":"1116136131797471274","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T22:42:26.417000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T22:42:26.417000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","1100941971633213500","1072591948499664996"]}},{"id":"1116129607104594050","type":0,"content":"<@1072591948499664996> how can I output slack markdown","channel_id":"1072944049788555314","author":{"id":"319769995603345409","username":"solr","global_name":null,"avatar":"e68ace9966558066d4600a2af7fb03f9","discriminator":"8375","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:20:35.019000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116129607104594050","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I output slack markdown","last_message_id":"1116129982004072559","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T22:20:35.161000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T22:20:35.161000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","319769995603345409","1072591948499664996"]}},{"id":"1116129415202611391","type":0,"content":"<@1072591948499664996> Can I use an LLMChain as a tool in an agent?","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T22:19:49.266000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116129415202611391","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Can I use an LLMChain as a tool in an agent?","last_message_id":"1116137126799622144","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T22:19:49.421000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T22:19:49.421000+00:00"},"message_count":20,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":20,"member_ids_preview":["437808476106784770","1072591948499664996","454324403161923604"]}},{"id":"1116118920563269722","type":0,"content":"<@1072591948499664996>  Is it possible to link together an LLMChain with an Agent? as in feeding the output of the LLMChain to the Agent and vice verca?","channel_id":"1072944049788555314","author":{"id":"579710526620434462","username":"SJ","global_name":null,"avatar":null,"discriminator":"1791","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T21:38:07.149000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116118920563269722","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is it possible to link together an LLMChain with an Agent? as in feeding the output of the","last_message_id":"1116119809982206054","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T21:38:07.310000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T21:38:07.310000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["579710526620434462","437808476106784770","1072591948499664996"]}},{"id":"1116109774480736368","type":0,"content":"<@1072591948499664996> I am using `AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, max_execution_time=15)` and I have the past messages in a variable. I want to give a chat history to the agent, is it possible? If so, please show me how.","channel_id":"1072944049788555314","author":{"id":"620349234633375797","username":"pcoronado","global_name":null,"avatar":"85d9a016581cbff6f9ee8f15930f1539","discriminator":"4338","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T21:01:46.553000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116109774480736368","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am using `AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, max","last_message_id":"1116119401473773708","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T21:01:46.754000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T21:01:46.754000+00:00"},"message_count":15,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":17,"member_ids_preview":["437808476106784770","1072591948499664996","620349234633375797"]}},{"id":"1116098920637018142","type":0,"content":"<@1072591948499664996> how can I implement ConversationSummaryMemory in Python?","channel_id":"1072944049788555314","author":{"id":"915229846592946237","username":"Niksta -------->>> Let‚Äôs go! üî•","global_name":null,"avatar":"421a127232684dfcfe183a2152384481","discriminator":"5365","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T20:18:38.795000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116098920637018142","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I implement ConversationSummaryMemory in Python?","last_message_id":"1116110808062111826","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T20:18:39.014000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T20:18:39.014000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["437808476106784770","915229846592946237","1072591948499664996"]}},{"id":"1116098249158311966","type":0,"content":"<@&1072943855747481672> is it possible to let the langchain agent to answer an unseen type of question with its own knowledge from the pre-trained embedding?","channel_id":"1072944049788555314","author":{"id":"1109905758453239848","username":"Goo","global_name":null,"avatar":null,"discriminator":"0633","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T20:15:58.702000+00:00","edited_timestamp":"2023-06-07T20:16:16.701000+00:00","flags":0,"components":[]},{"id":"1116092685057523822","type":0,"content":"<@1072591948499664996> does `langchainjs` `apichain` support `POST` or only `GET`?","channel_id":"1072944049788555314","author":{"id":"1111680188867153920","username":"simon.at.waterfield","global_name":null,"avatar":null,"discriminator":"6948","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T19:53:52.117000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116092685057523822","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"does `langchainjs` `apichain` support `POST` or only `GET`?","last_message_id":"1116092762165612564","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T19:53:52.762000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T19:53:52.762000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1111680188867153920","1072591948499664996","437808476106784770"]}},{"id":"1116082262396653668","type":0,"content":"<@1072591948499664996> how to use gpt-4 with langchain as chat model?","channel_id":"1072944049788555314","author":{"id":"782729216461897798","username":"vintro","global_name":null,"avatar":"df80ad1878ebab3dcd6ca590a8064884","discriminator":"1873","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T19:12:27.161000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116082262396653668","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to use gpt-4 with langchain as chat model?","last_message_id":"1116082416348569752","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T19:12:27.421000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T19:12:27.421000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["782729216461897798","1072591948499664996","437808476106784770"]}},{"id":"1116082181564022854","type":0,"content":"<@&1072943855747481672> how to use gpt-4 with langchain as chat model?","channel_id":"1072944049788555314","author":{"id":"782729216461897798","username":"vintro","global_name":null,"avatar":"df80ad1878ebab3dcd6ca590a8064884","discriminator":"1873","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T19:12:07.889000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116082117072404501","type":0,"content":"<@1072591948499664996> how to use gpt-4 with langchain as chat model?","channel_id":"1072944049788555314","author":{"id":"782729216461897798","username":"vintro","global_name":null,"avatar":"df80ad1878ebab3dcd6ca590a8064884","discriminator":"1873","public_flags":4194304,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T19:11:52.513000+00:00","edited_timestamp":"2023-06-07T19:12:00.527000+00:00","flags":0,"components":[]},{"id":"1116076037726552104","type":0,"content":"<@1072591948499664996> how can I run HuggingFacePipeline with my mps gpu device?","channel_id":"1072944049788555314","author":{"id":"691473791200526357","username":"lucasq","global_name":null,"avatar":null,"discriminator":"4179","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T18:47:43.084000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116076037726552104","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I run HuggingFacePipeline with my mps gpu device?","last_message_id":"1116076278336983071","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T18:47:43.378000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T18:47:43.378000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","691473791200526357"]}},{"id":"1116071884392509531","type":0,"content":"<@1072591948499664996> how to add a new agent when triggering a tool","channel_id":"1072944049788555314","author":{"id":"978269959631302666","username":"Daniel Becker","global_name":null,"avatar":"016b5bbf0f142e5ceb96f64421ce378f","discriminator":"0760","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T18:31:12.852000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116071884392509531","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how to add a new agent when triggering a tool","last_message_id":"1116072129549578310","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T18:31:13.128000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T18:31:13.128000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["978269959631302666","437808476106784770","1072591948499664996"]}},{"id":"1116070053385883678","type":0,"content":"<@1072591948499664996>  Is it possible to get a progress bar showing the process of ingesting from texts","channel_id":"1072944049788555314","author":{"id":"643831443260309514","username":"pandaboy","global_name":null,"avatar":"276a03cd23f4874fa3ee4e8472396ce7","discriminator":"9623","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T18:23:56.306000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116070053385883678","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is it possible to get a progress bar showing the process of ingesting from texts","last_message_id":"1116070766182674454","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T18:23:56.537000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T18:23:56.537000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","643831443260309514","1072591948499664996"]}},{"id":"1116068540609470527","type":0,"content":"<@1072591948499664996> I'm building a weather forecast tool/plugin. It has a single get endpoint, and a requried parameter of 'city'. If the user asks for the weather, but doesn't specify the city, how can I force the chatbot to ask for a city?","channel_id":"1072944049788555314","author":{"id":"256421922961752064","username":"RyanMcVitie","global_name":"Ryan","avatar":null,"discriminator":"4544","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T18:17:55.632000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116068540609470527","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I'm building a weather forecast tool/plugin. It has a single get endpoint, and a requried","last_message_id":"1116069513948053635","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T18:17:55.869000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T18:17:55.869000+00:00"},"message_count":8,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":8,"member_ids_preview":["1072591948499664996","256421922961752064","437808476106784770"]}},{"id":"1116067400962547712","type":0,"content":"<@1072591948499664996> How do I make it so I can have the chatbot also access a separate set of documents that is in a different format? \"from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\nembeddings = OpenAIEmbeddings()\nchroma_db = Chroma.from_documents(documents=documents, embedding=embeddings)\nllm = ChatOpenAI(model_name=\"gpt-4\")\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\nretriever = chroma_db.as_retriever()\n#retriever.search_kwargs = {'k':1}\n\nqa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\"","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T18:13:23.919000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116067400962547712","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How do I make it so I can have the chatbot also access a separate set of documents that is","last_message_id":"1116067697847959574","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T18:13:24.079000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T18:13:24.079000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","161206345620652032","437808476106784770"]}},{"id":"1116060641761972254","type":0,"content":"<@1072591948499664996> hi","channel_id":"1072944049788555314","author":{"id":"1087246900819939348","username":"helical_path","global_name":null,"avatar":null,"discriminator":"6553","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T17:46:32.400000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116060641761972254","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"hi","last_message_id":"1116060703636328560","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T17:46:32.974000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T17:46:32.974000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","1087246900819939348","437808476106784770"]}},{"id":"1116057257906544691","type":0,"content":"<@&1072943855747481672>  Can I use prompts when querying an api using create_openapi_agent? I would like to add some more context to the queries sent to the api, without having to include it every time.","channel_id":"1072944049788555314","author":{"id":"404019139473047554","username":"hazard567","global_name":null,"avatar":null,"discriminator":"0664","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T17:33:05.626000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116052110677651557","type":0,"content":"<@1072591948499664996> I have multiple .CSV documents, I want to get the passage of a specific document, how would you do that?","channel_id":"1072944049788555314","author":{"id":"620349234633375797","username":"pcoronado","global_name":null,"avatar":"85d9a016581cbff6f9ee8f15930f1539","discriminator":"4338","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T17:12:38.431000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116052110677651557","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have multiple .CSV documents, I want to get the passage of a specific document, how woul","last_message_id":"1116052743325499493","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T17:12:38.614000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T17:12:38.614000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["620349234633375797","1072591948499664996","437808476106784770"]}},{"id":"1116048867159449631","type":0,"content":"<@1072591948499664996> im creating an app that is heavily tool based, im using agent CHAT_ZERO_SHOT_REACT_DESCRIPTION, but I`m not being able to use memory with it, is that right? when i say my name and later ask my name the bot doesnt remember, what can I do to use CHAT_ZERO_SHOT_REACT_DESCRIPTION with memory?","channel_id":"1072944049788555314","author":{"id":"978269959631302666","username":"Daniel Becker","global_name":null,"avatar":"016b5bbf0f142e5ceb96f64421ce378f","discriminator":"0760","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:59:45.116000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116048867159449631","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"im creating an app that is heavily tool based, im using agent CHAT_ZERO_SHOT_REACT_DESCRIP","last_message_id":"1116049940540555395","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:59:45.400000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:59:45.400000+00:00"},"message_count":9,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","978269959631302666","437808476106784770"]}},{"id":"1116047957163253770","type":0,"content":"<@1072591948499664996> how do i use search_kwargs for a Chroma vector store?","channel_id":"1072944049788555314","author":{"id":"378791112736899073","username":"0xnuts","global_name":"0xnuts","avatar":"d2050091b956250174e2f8f6633c4f3a","discriminator":"1004","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:56:08.156000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116047957163253770","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i use search_kwargs for a Chroma vector store?","last_message_id":"1116360175373271082","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:56:08.294000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:56:08.294000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["378791112736899073","437808476106784770","1072591948499664996"]}},{"id":"1116045646810587249","type":0,"content":"<@1072591948499664996> what agent to use to force usage of tools","channel_id":"1072944049788555314","author":{"id":"978269959631302666","username":"Daniel Becker","global_name":null,"avatar":"016b5bbf0f142e5ceb96f64421ce378f","discriminator":"0760","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:46:57.325000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116045646810587249","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what agent to use to force usage of tools","last_message_id":"1116045812988907631","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:46:57.563000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:46:57.563000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","978269959631302666"]}},{"id":"1116040724794847273","type":0,"content":"<@1072591948499664996> How can I fix this: ValidationError: 2 validation errors for OpenAIEmbeddings\ndocument_model_name\n  extra fields not permitted (type=value_error.extra)\nquery_model_name\n  extra fields not permitted (type=value_error.extra)","channel_id":"1072944049788555314","author":{"id":"702981662543315054","username":"yabramuvdi","global_name":null,"avatar":"3c976f39558e6e38b92530ad367d1f61","discriminator":"5138","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:27:23.825000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116040724794847273","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I fix this: ValidationError: 2 validation errors for OpenAIEmbeddingsdocument_mod","last_message_id":"1116044483126116436","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:27:23.994000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:27:23.994000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["437808476106784770","702981662543315054","1072591948499664996"]}},{"id":"1116037345473462372","type":0,"content":"<@1072591948499664996>  can I use an if when I am creating a new agent?","channel_id":"1072944049788555314","author":{"id":"383746001745477633","username":"FBasualdo","global_name":null,"avatar":"4c5679ba27dbf0f3025f9f791a9339bd","discriminator":"5920","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:13:58.132000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116037345473462372","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"can I use an if when I am creating a new agent?","last_message_id":"1116187908471259147","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:13:58.456000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:13:58.456000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["437808476106784770","383746001745477633","1072591948499664996"]}},{"id":"1116036914806542356","type":0,"content":"<@1072591948499664996> doc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\nFor chain_type, what other options do I have?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T16:12:15.453000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116036914806542356","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"doc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")For chain_type, what other options","last_message_id":"1116037187885076502","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T16:12:15.739000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T16:12:15.739000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["161206345620652032","437808476106784770","1072591948499664996"]}},{"id":"1116033487099662506","type":0,"content":"<@1072591948499664996> this is my code:\n   retriever = chroma_Vectorstore.as_retriever(qa_template=QA_PROMPT_DOCUMENT_CHAT, search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.2})\n                self.chain = ConversationalRetrievalChain.from_llm(self.llm, retriever,\n                                                                    return_source_documents=True,verbose=True, \n                                                                    memory=self.memory)\n                \n                # docs = retriever.get_relevant_documents('What are aspects in data science')\n                # print('TUUUKAAAJ', docs)\n\n                result = self.chain({\"question\": question})\nhow can i pass the QA_PROMPT_DOCUMENT_CHAT to be used, because current code doesn't use the template","channel_id":"1072944049788555314","author":{"id":"376809451044470785","username":"Ziigaa","global_name":null,"avatar":"92db54d6a0cfb151255c4fcc16ce9c9e","discriminator":"7727","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:58:38.224000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116033487099662506","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"this is my code:   retriever = chroma_Vectorstore.as_retriever(qa_template=QA_PROMPT_DOCU","last_message_id":"1116033708059799613","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:58:38.449000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:58:38.449000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["376809451044470785","1072591948499664996","437808476106784770"]}},{"id":"1116032944352530452","type":0,"content":"<@1072591948499664996> \nMy code:\n```\n# Set up the turbo LIM\nturbo_llm = ChatOpenAI(\n    temperature=0,\n    model_name='gpt-3.5-turbo'\n)\n\n\ndef meaning_of_life(input=\"\"):\n    return 'The meaning of life is 42 if rounded but is actually 42.17658'\n\n\ndef random_num(input=\"\"):\n    return random.randint(0, 5)\n\n\nlife_tool = Tool(\n    name='Meaning of Life',\n    func=meaning_of_life,\n    description=\"Useful for when you need to answer questions about the meaning of life. input should be MOL \"\n)\n\nrandom_tool = Tool(\n    name='Random number',\n    func=random_num,\n    description=\"Useful for when you need to get a random number. input should be 'random'\"\n)\n\n\ntools = [random_tool, life_tool]\n# conversational agent memory\nmemory = ConversationBufferWindowMemory(\n    memory_key='chat history',\n    k=3,\n    return_messages=True\n)\n# create our agent\nconversational_agent = initialize_agent(\n    agent='chat-conversational-react-description',\n    tools=tools,\n    llm=turbo_llm,\n    verbose=True,\n    max_iterations=3,\n    early_stopping_method='generate',\n    memory=memory\n)\n\noutput = conversational_agent(\"What is the meaning of life?\")\n\nprint(output)\n```\n\nThe error:\n```\nValueError: A single string input was passed in, but this chain expects multiple inputs ({'chat_history', 'input'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`\n```","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:56:28.823000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116032944352530452","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"My code:```# Set up the turbo LIMturbo_llm = ChatOpenAI(    temperature=0,    model_n","last_message_id":"1116123353774817371","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:56:28.965000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:56:28.965000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","454324403161923604","437808476106784770"]}},{"id":"1116032413563359252","type":0,"content":"<@1072591948499664996> I got this error in my code. Please help.\n```ValueError: A single string input was passed in, but this chain expects multiple inputs ({'chat_history', 'input'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})```","channel_id":"1072944049788555314","author":{"id":"454324403161923604","username":"ImAcyborg","global_name":null,"avatar":null,"discriminator":"9953","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:54:22.273000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116032413563359252","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I got this error in my code. Please help.```ValueError: A single string input was passed","last_message_id":"1116032807148453908","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:54:23.066000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:54:23.066000+00:00"},"message_count":6,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":6,"member_ids_preview":["1072591948499664996","437808476106784770","454324403161923604"]}},{"id":"1116028661452259448","type":0,"content":"<@1072591948499664996> how can I get my `agent` to use more than one `Tool` object when I make a query?","channel_id":"1072944049788555314","author":{"id":"100821635820429312","username":"ryanglambert","global_name":null,"avatar":null,"discriminator":"9301","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:39:27.700000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116028661452259448","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I get my `agent` to use more than one `Tool` object when I make a query?","last_message_id":"1116028902456971324","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:39:27.901000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:39:27.901000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["100821635820429312","1072591948499664996","437808476106784770"]}},{"id":"1116026711088308274","type":0,"content":"Can langchain run autogpt with a different LLM?","channel_id":"1072944049788555314","author":{"id":"1105347711030542376","username":"learncrave","global_name":null,"avatar":"519a0e95d6740ac7c97c3082f1ae9de0","discriminator":"4972","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:31:42.697000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1116025159657193512","type":0,"content":"<@1072591948499664996> \nIn this code, what does `df` mean?\n\n```\n# overall_chain: input= Review \n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=[\"Review\"],\n    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n    verbose=True\n)\nreview = df.Review[5]\noverall_chain(review)\n```","channel_id":"1072944049788555314","author":{"id":"576645852177760267","username":"EvanY","global_name":null,"avatar":"17a49cf5a11229c7c8c56fc07b481a83","discriminator":"8955","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:25:32.807000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116025159657193512","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"In this code, what does `df` mean?```# overall_chain: input= Review # and output= Engl","last_message_id":"1116027080380010586","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:25:33.108000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:25:33.108000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["576645852177760267","437808476106784770","1072591948499664996"]}},{"id":"1116023092519637042","type":0,"content":"<@1072591948499664996> I am using `langchainjs`, provide a `\"count token\"` function","channel_id":"1072944049788555314","author":{"id":"1111680188867153920","username":"simon.at.waterfield","global_name":null,"avatar":null,"discriminator":"6948","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:17:19.963000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116023092519637042","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am using `langchainjs`, provide a `\"count token\"` function","last_message_id":"1116023187361247272","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:17:20.129000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:17:20.129000+00:00"},"message_count":2,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":2,"member_ids_preview":["437808476106784770","1072591948499664996","1111680188867153920"]}},{"id":"1116022417324781850","type":0,"content":"<@1072591948499664996> Is it possible to combine multiple conversational agents? For example, I need to develop an agent that manages multiple prompts and it will use multiple tools and documents to retrieve an information and give a response. For example, if the user asks \"What is the price of a Nestl√© chocolate\", I want the agent to behavior  as a chocolate seller (from a prompt) and retrieve information about chocolate prices (from the chocolate.csv spreadsheet). How can I do that? Please provide me a guide on how to setup the chain.","channel_id":"1072944049788555314","author":{"id":"620349234633375797","username":"pcoronado","global_name":null,"avatar":"85d9a016581cbff6f9ee8f15930f1539","discriminator":"4338","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T15:14:38.984000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116022417324781850","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Is it possible to combine multiple conversational agents? For example, I need to develop a","last_message_id":"1116022589601611776","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T15:14:39.181000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T15:14:39.181000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","620349234633375797","437808476106784770"]}},{"id":"1116014061059059724","type":0,"content":"<@1072591948499664996> How can I make my retriever retrieve all my documents without exceeding the token limit?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T14:41:26.695000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116014061059059724","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I make my retriever retrieve all my documents without exceeding the token limit?","last_message_id":"1116015915851595787","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T14:41:27.006000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T14:41:27.006000+00:00"},"message_count":10,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":10,"member_ids_preview":["1072591948499664996","437808476106784770","161206345620652032"]}},{"id":"1116009961626009662","type":0,"content":"<@1072591948499664996> i have the following use case : i want to have a knowledge base that is formed from 2 or more CSV files , with different schemes (different column names)\nto answer questions that are crossed the different csv data , i need to map between the different columns in different csv's.\nhow can i do this mapping ?\ni thought about creating additional mapping txt file , and have it as a tool used to get the mapping, but when it run it knows noghting about the csv's...\ndo i have option to do it in sequence ? like : \n1. look for the mapping\n2. get the csv's scheme\n3. map the columns to be with the same names\n4. join the csv's data according to the common columns\n5. answer the question on the unified dataframe","channel_id":"1072944049788555314","author":{"id":"1061886480974352394","username":"yoyotheone","global_name":null,"avatar":null,"discriminator":"4786","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T14:25:09.314000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116009961626009662","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i have the following use case : i want to have a knowledge base that is formed from 2 or m","last_message_id":"1116011528903209001","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T14:25:09.466000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T14:25:09.466000+00:00"},"message_count":5,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":5,"member_ids_preview":["437808476106784770","1072591948499664996","1061886480974352394"]}},{"id":"1116002418161893516","type":0,"content":"<@1072591948499664996>  while using ConversationalRetrievalChain how can i retrive multiple chuck of same document where qdrant is my vectore db","channel_id":"1072944049788555314","author":{"id":"458856341897740310","username":"yashs","global_name":null,"avatar":null,"discriminator":"8846","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T13:55:10.812000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1116002418161893516","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"while using ConversationalRetrievalChain how can i retrive multiple chuck of same document","last_message_id":"1116002629940674620","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T13:55:11.077000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T13:55:11.077000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["458856341897740310","437808476106784770","1072591948499664996"]}},{"id":"1115998965943521382","type":0,"content":"<@1072591948499664996> how can I real a fewshot template from yaml with examples selector?","channel_id":"1072944049788555314","author":{"id":"236536688082419713","username":"Nikazzio","global_name":"Nikazzio","avatar":"7ca3fa8c668c42b01016ba5eb2d0b0de","discriminator":"0584","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T13:41:27.739000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115998965943521382","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I real a fewshot template from yaml with examples selector?","last_message_id":"1116000707980578827","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T13:41:27.995000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T13:41:27.995000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":12,"member_ids_preview":["437808476106784770","1072591948499664996","236536688082419713"]}},{"id":"1115994694304944128","type":0,"content":"<@1072591948499664996> I keep getting \"InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 78597 tokens. Please reduce the length of the messages.\". My issue is I have a lot of data that I need the chatbot to have for context, how do I make sure it has access to all that without bypassing the token limit?","channel_id":"1072944049788555314","author":{"id":"161206345620652032","username":"yaxan","global_name":"Yeezus","avatar":"729af41237073dd98eacd106507ab5f1","discriminator":"0","public_flags":256,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T13:24:29.301000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115994694304944128","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I keep getting \"InvalidRequestError: This model's maximum context length is 8192 tokens. H","last_message_id":"1115994913566367744","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T13:24:29.498000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T13:24:29.498000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["161206345620652032","437808476106784770","1072591948499664996"]}},{"id":"1115986977632813066","type":0,"content":"<@1072591948499664996> I am connecting a large language model to my BigQuery data using an SQL agent.  I have many tables and when I make a query the LLM  first lists all the table names, but there are so many tables that the LLM reaches the context limit. How can I resolve this issue and query the desired tables efficiently?","channel_id":"1072944049788555314","author":{"id":"236901446803652609","username":"Plain Filth","global_name":null,"avatar":"8221ce42c808325ab2106494b7e58e40","discriminator":"3586","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:53:49.503000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115986977632813066","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I am connecting a large language model to my BigQuery data using an SQL agent.  I have man","last_message_id":"1116339585669144647","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:53:49.665000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:53:49.665000+00:00"},"message_count":19,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":19,"member_ids_preview":["1072591948499664996","437808476106784770","236901446803652609"]}},{"id":"1115979399624409188","type":0,"content":"<@1072591948499664996> why should i use map_reduce over stuff chain_type?","channel_id":"1072944049788555314","author":{"id":"378791112736899073","username":"0xnuts","global_name":"0xnuts","avatar":"d2050091b956250174e2f8f6633c4f3a","discriminator":"1004","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:23:42.765000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115979399624409188","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"why should i use map_reduce over stuff chain_type?","last_message_id":"1115979604126081094","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:23:42.951000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:23:42.951000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","378791112736899073","1072591948499664996"]}},{"id":"1115978930508279928","type":0,"content":"<@1072591948499664996> Right now my code looks like this\n\nfrom langchain.prompts import PromptTemplate\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n{context}\n\nQuestion: {question} \nAlways answer in Danish:\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\nchain_type_kwargs = {\"prompt\": PROMPT}\n# Igangs√¶t chain\nqa = RetrievalQA.from_chain_type(\n    llm=openai_instance, \n    chain_type=\"stuff\", \n    retriever=docsearch.as_retriever(), \n    return_source_documents=True, \n    chain_type_kwargs=chain_type_kwargs)\n\nresult = qa({\"query\": query})\n\nThe code runs perfectly, but it does not listen to the prompt. How can I ensure that the custom prompt gets included?","channel_id":"1072944049788555314","author":{"id":"356441776141107202","username":"aldarra","global_name":null,"avatar":null,"discriminator":"7825","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:21:50.919000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115978930508279928","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Right now my code looks like thisfrom langchain.prompts import PromptTemplateprompt_tem","last_message_id":"1115985835058274345","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:21:51.036000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:21:51.036000+00:00"},"message_count":15,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":15,"member_ids_preview":["356441776141107202","1072591948499664996","437808476106784770"]}},{"id":"1115977422207201340","type":0,"content":"<@1072591948499664996>  How can I  parse an output within a ConversationChain to trigger an sql query?","channel_id":"1072944049788555314","author":{"id":"903249443946635285","username":"mateusexel","global_name":null,"avatar":null,"discriminator":"9148","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:15:51.312000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115977422207201340","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How can I  parse an output within a ConversationChain to trigger an sql query?","last_message_id":"1115977879944187924","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:15:51.576000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:15:51.576000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["903249443946635285","437808476106784770","1072591948499664996"]}},{"id":"1115977012801192017","type":0,"content":"<@1072591948499664996> How to add memeory to vector db Q&A with source chain?","channel_id":"1072944049788555314","author":{"id":"962398456792612886","username":"lemig","global_name":null,"avatar":null,"discriminator":"8990","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:14:13.702000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115977012801192017","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"How to add memeory to vector db Q&A with source chain?","last_message_id":"1115977396961681438","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:14:13.880000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:14:13.880000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["437808476106784770","962398456792612886","1072591948499664996"]}},{"id":"1115975080208515133","type":0,"content":"<@1072591948499664996> while storing documents into qdrant vector db my documents are stored into chunks suppose i have 1 text document it is store as 3 vectors into qdrant is it possible to store single vector for one document","channel_id":"1072944049788555314","author":{"id":"458856341897740310","username":"yashs","global_name":null,"avatar":null,"discriminator":"8846","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T12:06:32.936000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115975080208515133","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"while storing documents into qdrant vector db my documents are stored into chunks suppose","last_message_id":"1115997445785141258","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T12:06:33.102000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T12:06:33.102000+00:00"},"message_count":22,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":22,"member_ids_preview":["458856341897740310","1072591948499664996","437808476106784770"]}},{"id":"1115972038125961318","type":0,"content":"<@1072591948499664996> I have a sequence of 3 chains in a SequentialChain\n\nSequentialChain(chains=[chain1, chain2, chain3])\n\nChain1 has input x, and output xb\nChain2 has input xb, and outputs xc\nChain3 has input x, xc, and z\n\nIf the sequentialchains input variables look like [x, z] I get an error  \"Missing required input keys: {xb}\"\n\nBut if I add xb so the input variables are [x, z, xb], I get an error \"Chain returned keys that already exist: {xb}\"\n\nHow do I fix this in python so that variables from the chain are passed to the next one?","channel_id":"1072944049788555314","author":{"id":"276303869938761728","username":"Swiggie","global_name":null,"avatar":"33398a44b472701025bd57bb11477771","discriminator":"8439","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:54:27.647000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115972038125961318","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"I have a sequence of 3 chains in a SequentialChainSequentialChain(chains=[chain1, chain2","last_message_id":"1115972489655369779","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T11:54:27.883000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T11:54:27.883000+00:00"},"message_count":4,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":4,"member_ids_preview":["1072591948499664996","276303869938761728","437808476106784770"]}},{"id":"1115966737771417690","type":0,"content":"<@1072591948499664996> how can I incorporate unique session IDs into something like mongoDB based memory, so that a user's particular chat history is retained but is unique each time...","channel_id":"1072944049788555314","author":{"id":"230655927538286593","username":"theheffalump00","global_name":"The Heffa","avatar":"420d49840e3f1ee0f21b573ed39d839b","discriminator":"0","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:33:23.944000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115966737771417690","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I incorporate unique session IDs into something like mongoDB based memory, so that","last_message_id":"1115969703916093480","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T11:33:24.129000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T11:33:24.129000+00:00"},"message_count":13,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":14,"member_ids_preview":["1072591948499664996","437808476106784770","230655927538286593"]}},{"id":"1115964700895744050","type":0,"content":"<@1072591948499664996> it's an error using the pdf-loader in the langchain library and the nextjs framework when uploading to vercel, help me debug why it's happening, why is fs clashing between the two. It seems it's only on the server side that it happens, is it because my service is calling the langchain instead of the direct api route? what could be the reason?\nFailed to compile.\n./node_modules/pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\nModule not found: Can't resolve 'fs'\nModule build failed: UnhandledSchemeError: Reading from \"node:fs/promises\" is not handled by plugins (Unhandled scheme).\nWebpack supports \"data:\" and \"file:\" URIs by default.\nYou may need an additional plugin to handle \"node:\" URIs.\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395974\n    at Hook.eval [as callAsync] (eval at create (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:28771), <anonymous>:6:1)\n    at Hook.CALL_ASYNC_DELEGATE [as _callAsync] (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:25925)\n    at Object.processResource (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395899)\n    at processResource (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:5308)\n    at iteratePitchingLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:4667)\n    at runLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:8590)\n    at NormalModule._doBuild (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395761)\n    at NormalModule.build (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:397789)\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:81243","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:25:18.315000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115964700895744050","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"it's an error using the pdf-loader in the langchain library and the nextjs framework when","last_message_id":"1115964890251792414","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T11:25:18.445000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T11:25:18.445000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","706926250723115019","1072591948499664996"]}},{"id":"1115964659313422427","type":0,"content":"<@&1072943855747481672> it's an error using the pdf-loader in the langchain library and the nextjs framework when uploading to vercel, help me debug why it's happening, why is fs clashing between the two. It seems it's only on the server side that it happens, is it because my service is calling the langchain instead of the direct api route? what could be the reason?\nFailed to compile.\n./node_modules/pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\nModule not found: Can't resolve 'fs'\nModule build failed: UnhandledSchemeError: Reading from \"node:fs/promises\" is not handled by plugins (Unhandled scheme).\nWebpack supports \"data:\" and \"file:\" URIs by default.\nYou may need an additional plugin to handle \"node:\" URIs.\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395974\n    at Hook.eval [as callAsync] (eval at create (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:28771), <anonymous>:6:1)\n    at Hook.CALL_ASYNC_DELEGATE [as _callAsync] (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:25925)\n    at Object.processResource (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395899)\n    at processResource (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:5308)\n    at iteratePitchingLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:4667)\n    at runLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:8590)\n    at NormalModule._doBuild (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395761)\n    at NormalModule.build (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:397789)\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:81243","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:25:08.401000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1115962672098324490","type":0,"content":"<@1072591948499664996> how do i resolve this? ValidationError: 1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. 'batch_id' (type=value_error)","channel_id":"1072944049788555314","author":{"id":"899634011863941130","username":"adidor","global_name":null,"avatar":null,"discriminator":"8452","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:17:14.612000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115962672098324490","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i resolve this? ValidationError: 1 validation error for PromptTemplate__root__  I","last_message_id":"1115962773659189279","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T11:17:14.840000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T11:17:14.840000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","899634011863941130"]}},{"id":"1115959412507541554","type":0,"content":"<@&1072943855747481672>  I am doing Retrieval Question Answering with Sources. How can I implement memory, so that a question takes previous conversation as context?","channel_id":"1072944049788555314","author":{"id":"962398456792612886","username":"lemig","global_name":null,"avatar":null,"discriminator":"8990","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:04:17.465000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1115958928002531352","type":0,"content":"<@1072591948499664996> , I have 5 documents which I am feeding to recursive splitter. When it outputs, some documents are dupicated. why is that?","channel_id":"1072944049788555314","author":{"id":"301114343322550274","username":"QWERTY","global_name":null,"avatar":"04eaad71c58303bf4677c4ccc597e8f5","discriminator":"8948","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T11:02:21.950000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115958928002531352","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":", I have 5 documents which I am feeding to recursive splitter. When it outputs, some docum","last_message_id":"1115959120865013810","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T11:02:22.218000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T11:02:22.218000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","301114343322550274","437808476106784770"]}},{"id":"1115956323083878442","type":0,"content":"<@1072591948499664996> how can I use ConversationBufferWindowMemory() with a SequentialChain, with the LLMChain with memory being the second chain in the overall chain?","channel_id":"1072944049788555314","author":{"id":"276303869938761728","username":"Swiggie","global_name":null,"avatar":"33398a44b472701025bd57bb11477771","discriminator":"8439","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:52:00.889000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115956323083878442","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can I use ConversationBufferWindowMemory() with a SequentialChain, with the LLMChain w","last_message_id":"1115956463861506089","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T10:52:01.355000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T10:52:01.355000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["1072591948499664996","437808476106784770","276303869938761728"]}},{"id":"1115952904944230491","type":0,"content":"<@1072591948499664996> below is my logic for chain call\n\n    const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n      SystemMessagePromptTemplate.fromTemplate(\n        renderedPrompt,\n      ),\n      HumanMessagePromptTemplate.fromTemplate('{human_input}'),\n    ]);\n    const chat = new ChatOpenAI({\n      ...azureBaseConfig,\n      callbackManager: CallbackManager.fromHandlers({\n        async handleLLMNewToken(token: string) {\n          process.stdout.write(token);\n        },\n      }),\n      maxTokens,\n    });\n    const chainB = new LLMChain({\n      prompt: chatPrompt,\n      llm: chat,\n    });\nconst result = await chainB.call({\n  human_input: 'Generate content',\n});\n\nI'd like check the output (result variable) for specific headings and regenerate the output (result). The chainB.call needs to run again if the output fails the conditional check.","channel_id":"1072944049788555314","author":{"id":"782985905686052875","username":"vidyarthi","global_name":null,"avatar":"0dce1a8a5750d1e57f7737c0870f3806","discriminator":"0013","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:38:25.941000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115952904944230491","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"below is my logic for chain call    const chatPrompt = ChatPromptTemplate.fromPromptMess","last_message_id":"1115953154895392769","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T10:38:26.146000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T10:38:26.146000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["782985905686052875","1072591948499664996","437808476106784770"]}},{"id":"1115950540384391250","type":0,"content":"<@1072591948499664996>  i have this:\n                past_messages = [\n                    HumanMessage(content = \"My name's Jonas\"),\n                    AIMessage(content = \"Nice to meet you, Jonas!\"),\n                ]\n\n\nnow how to add in ConversationBufferMemory?","channel_id":"1072944049788555314","author":{"id":"1077102711025184819","username":"muhammad_daniyal","global_name":null,"avatar":null,"discriminator":"6245","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:29:02.186000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115950540384391250","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i have this:                past_messages = [                    HumanMessage(content =","last_message_id":"1115950672916000828","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T10:29:02.812000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T10:29:02.812000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["437808476106784770","1077102711025184819","1072591948499664996"]}},{"id":"1115947430328946799","type":0,"content":"This is true with the two integrations I tried to load the LLMs locally i.e. gpt4all and llama-cpp-python","channel_id":"1072944049788555314","author":{"id":"1111481652191957054","username":"cryptoescobar","global_name":null,"avatar":"1628fc11e7961d85181295493426b775","discriminator":"6435","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:16:40.691000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1115947243376234496","type":0,"content":"Hey there, I have been trying to build a chat UI using a UI library, but there's a problem whenever there's a long response from the LlmChain i.e. it reloads the model","channel_id":"1072944049788555314","author":{"id":"1111481652191957054","username":"cryptoescobar","global_name":null,"avatar":"1628fc11e7961d85181295493426b775","discriminator":"6435","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:15:56.118000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1115946677916938352","type":0,"content":"<@1072591948499664996> i want to add human message and AIMessage into ConversationBufferMEmory manually","channel_id":"1072944049788555314","author":{"id":"1077102711025184819","username":"muhammad_daniyal","global_name":null,"avatar":null,"discriminator":"6245","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T10:13:41.302000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115946677916938352","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"i want to add human message and AIMessage into ConversationBufferMEmory manually","last_message_id":"1115947734168522802","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T10:13:41.512000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T10:13:41.512000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","1077102711025184819"]}},{"id":"1115942066711695381","type":0,"content":"<@1072591948499664996> Failed to compile.\n./node_modules/pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\nModule not found: Can't resolve 'fs'\nModule build failed: UnhandledSchemeError: Reading from \"node:fs/promises\" is not handled by plugins (Unhandled scheme).\nWebpack supports \"data:\" and \"file:\" URIs by default.\nYou may need an additional plugin to handle \"node:\" URIs.\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395974\n    at Hook.eval [as callAsync] (eval at create (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:28771), <anonymous>:6:1)\n    at Hook.CALL_ASYNC_DELEGATE [as _callAsync] (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:13:25925)\n    at Object.processResource (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395899)\n    at processResource (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:5308)\n    at iteratePitchingLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:4667)\n    at runLoaders (/vercel/path0/node_modules/next/dist/compiled/loader-runner/LoaderRunner.js:1:8590)\n    at NormalModule._doBuild (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:395761)\n    at NormalModule.build (/vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:397789)\n    at /vercel/path0/node_modules/next/dist/compiled/webpack/bundle5.js:28:81243\nError: Command \"npm run build\" exited with 1\nBUILD_UTILS_SPAWN_1: Command \"npm run build\" exited with 1","channel_id":"1072944049788555314","author":{"id":"706926250723115019","username":"Woover","global_name":"Yehonatan Yosefi","avatar":"c96a0eec06c6b0aeb84f2193ba81d65a","discriminator":"8079","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T09:55:21.905000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115942066711695381","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Failed to compile../node_modules/pdf-parse/lib/pdf.js/v1.10.100/build/pdf.jsModule not f","last_message_id":"1115964019463958538","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T09:55:22.213000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T09:55:22.213000+00:00"},"message_count":15,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":15,"member_ids_preview":["706926250723115019","437808476106784770","1072591948499664996"]}},{"id":"1115940864485765200","type":0,"content":"<@1072591948499664996> Why does my chain only run one iteration then finish? toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=sync_browser)\ntools = toolkit.get_tools()\nchat_history = MessagesPlaceholder(variable_name=\"chat_history\")\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\nagent_chain = initialize_agent(\n    tools, \n    llm, \n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n    verbose=True, \n    memory=memory, \n    agent_kwargs = {\n        \"memory_prompts\": [chat_history],\n        \"input_variables\": [\"input\", \"agent_scratchpad\", \"chat_history\"]\n    }\n)","channel_id":"1072944049788555314","author":{"id":"879079705540714548","username":"meowmix","global_name":null,"avatar":"402264bf40d695292ec8f20cdac76c8a","discriminator":"8975","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T09:50:35.272000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115940864485765200","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Why does my chain only run one iteration then finish? toolkit = PlayWrightBrowserToolkit.f","last_message_id":"1115942034537201744","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T09:50:35.480000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T09:50:35.480000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["1072591948499664996","437808476106784770","879079705540714548"]}},{"id":"1115939200328548425","type":0,"content":"<@1072591948499664996> Hi how can i use filters in qdrant as retriever? I know i can pass filters by \n\"retriever.search_kwargs['filter'] = query_filter\"\n but what is the value/structure of 'query_filter' suppose i want to match key with value?","channel_id":"1072944049788555314","author":{"id":"458856341897740310","username":"yashs","global_name":null,"avatar":null,"discriminator":"8846","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T09:43:58.506000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115939200328548425","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Hi how can i use filters in qdrant as retriever? I know i can pass filters by \"retriever.","last_message_id":"1115941074452631585","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T09:43:58.715000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T09:43:58.715000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":9,"member_ids_preview":["1072591948499664996","458856341897740310","437808476106784770"]}},{"id":"1115938261156769864","type":0,"content":"<@1072591948499664996> using the LangChain javascript library, how can I add an array of documents to my pinecone vector database?","channel_id":"1072944049788555314","author":{"id":"660222849025441792","username":"Vinny","global_name":null,"avatar":"347ffe49630cd5713f19f22eefd75bd6","discriminator":"8401","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T09:40:14.590000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115938261156769864","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"using the LangChain javascript library, how can I add an array of documents to my pinecone","last_message_id":"1115941372101402624","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T09:40:14.851000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T09:40:14.851000+00:00"},"message_count":12,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":13,"member_ids_preview":["1072591948499664996","660222849025441792","437808476106784770"]}},{"id":"1115919757468782633","type":0,"content":"<@1072591948499664996> what does this error message mean:\n\"Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\" in the context of running this code:\n```\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.llms import OpenAI\n\npersist_directory = 'db'\n\nembeddings = OpenAIEmbeddings()\n\n# Now we can load the persisted database from disk, and use it as normal. \nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n\n\n# Create a memory object to track inputs/outputs and hold a conversation\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n\n\n# Initialize the ConversationalRetrievalChain\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectordb.as_retriever(), memory=memory)\n\nwhile True:\n    # Start a conversation by asking a question\n    query = input(\"Enter your question: \")\n    result = qa({\"question\": query})\n    print(result[\"answer\"])\n\n```","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T08:26:42.967000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115919757468782633","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"what does this error message mean:\"Retrying langchain.llms.openai.completion_with_retry.<","last_message_id":"1115928356068790282","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T08:26:43.148000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T08:26:43.148000+00:00"},"message_count":13,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":13,"member_ids_preview":["437808476106784770","839215717693259816","1072591948499664996"]}},{"id":"1115915582102048798","type":0,"content":"<@1072591948499664996> how do i define the metadata when using add_texts instead of add_documents when adding text to my pinecone vector store? Im using python","channel_id":"1072944049788555314","author":{"id":"835800670308794378","username":"therealdil","global_name":null,"avatar":"93a55a7192604f26027c124b5902c7e8","discriminator":"3786","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T08:10:07.482000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115915582102048798","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how do i define the metadata when using add_texts instead of add_documents when adding tex","last_message_id":"1115915787924930580","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T08:10:07.802000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T08:10:07.802000+00:00"},"message_count":3,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":3,"member_ids_preview":["835800670308794378","437808476106784770","1072591948499664996"]}},{"id":"1115915157110018100","type":0,"content":"<@&1072943855747481672> is there a downside to using add_documents for adding short, long texts, documents to the vector store, instead of using add_texts?","channel_id":"1072944049788555314","author":{"id":"835800670308794378","username":"therealdil","global_name":null,"avatar":"93a55a7192604f26027c124b5902c7e8","discriminator":"3786","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T08:08:26.156000+00:00","edited_timestamp":null,"flags":0,"components":[]},{"id":"1115914726501793835","type":0,"content":"<@1072591948499664996> how can i fix this error?\n\nerror OutputParserException [Error]: Failed to parse. Text: \"\n```json\n{\n        \"destination\": \"\" // answer to the question 'where do you want to go?'\n        \"duration\": \"\" // answer to the question 'how long do you want to stay?'\n        \"spend\": \"\" // answer to the question 'how much do you want to spend?'\n        \"party_size\": \"\" // answer to the question 'how many people are traveling with you?'\n        \"amusement\": \"\" // answer to the question 'do you want to visit a mountain, beach, or city?'\n}\n```\". Error: SyntaxError: Expected ',' or '}' after property value in JSON at position 21","channel_id":"1072944049788555314","author":{"id":"275866688933724160","username":"Damis","global_name":null,"avatar":"6865f487c315963b8929731510d52a2d","discriminator":"9861","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T08:06:43.491000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115914726501793835","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"how can i fix this error?error OutputParserException [Error]: Failed to parse. Text: \"`","last_message_id":"1115922773592768582","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T08:06:43.816000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T08:06:43.816000+00:00"},"message_count":10,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":10,"member_ids_preview":["275866688933724160","437808476106784770","1072591948499664996"]}},{"id":"1115913682245926952","type":0,"content":"<@1072591948499664996> in chroma what is the difference between similarity search and retrieval?","channel_id":"1072944049788555314","author":{"id":"839215717693259816","username":"Mat99","global_name":null,"avatar":"1ab6f335a89031c28d265edb5ead85cb","discriminator":"0427","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T08:02:34.521000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115913682245926952","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"in chroma what is the difference between similarity search and retrieval?","last_message_id":"1115919351497883671","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T08:02:34.651000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T08:02:34.651000+00:00"},"message_count":11,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":11,"member_ids_preview":["437808476106784770","1072591948499664996","839215717693259816"]}},{"id":"1115910220997218446","type":0,"content":"<@1072591948499664996> Why do I get an AssertionError when running a chain.arun({}) instead of something like this: InvalidRequestError(message=\"This model's maximum context length is 4097 tokens. However, your messages resulted in 15631 tokens. Please reduce the length of the messages.\", param='messages', code='context_length_exceeded', http_status=400, request_id=None)","channel_id":"1072944049788555314","author":{"id":"688471172073259033","username":"djquesadilla","global_name":null,"avatar":"d8947258d0191f891c4154c10cc856e4","discriminator":"5219","public_flags":0,"avatar_decoration":null},"attachments":[],"embeds":[],"mentions":[{"id":"1072591948499664996","username":"kapa.ai","avatar":"09e7b0315ef53d57936eb7d461f40224","discriminator":"2237","public_flags":0,"flags":0,"bot":true,"banner":null,"accent_color":null,"global_name":null,"avatar_decoration":null,"display_name":null,"banner_color":null}],"mention_roles":[],"pinned":false,"mention_everyone":false,"tts":false,"timestamp":"2023-06-07T07:48:49.295000+00:00","edited_timestamp":null,"flags":32,"components":[],"thread":{"id":"1115910220997218446","guild_id":"1038097195422978059","parent_id":"1072944049788555314","owner_id":"1072591948499664996","type":11,"name":"Why do I get an AssertionError when running a chain.arun({}) instead of something like thi","last_message_id":"1115912000262901850","thread_metadata":{"archived":false,"archive_timestamp":"2023-06-07T07:48:49.531000+00:00","auto_archive_duration":1440,"locked":false,"create_timestamp":"2023-06-07T07:48:49.531000+00:00"},"message_count":7,"member_count":3,"rate_limit_per_user":0,"flags":0,"total_message_sent":7,"member_ids_preview":["437808476106784770","688471172073259033","1072591948499664996"]}}]